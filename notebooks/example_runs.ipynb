{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This Notebook will demonstrate different configurations and executions of LLM-Analyst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sytem level imports\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "sys.path.append('/Users/dan/Code/LLM/llm_analyst')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import llm_analyst content in one cell to make the rest of the code cleaner.\n",
    "from llm_analyst.core.config import Config, DataSource\n",
    "from llm_analyst.core.research_analyst import LLMAnalyst\n",
    "from llm_analyst.core.research_editor import LLMEditor\n",
    "from llm_analyst.core.research_publisher import LLMPublisher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstrate running LLM-Analyst on Local Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'internet_search': <function llm_analyst.search_methods.internet_search.ddg_search(query, max_results=5)>,\n",
       " 'embedding_provider': OpenAIEmbeddings(client=<openai.resources.embeddings.Embeddings object at 0x132ddf1d0>, async_client=<openai.resources.embeddings.AsyncEmbeddings object at 0x10d5b1b90>, model='text-embedding-ada-002', dimensions=None, deployment='text-embedding-ada-002', openai_api_version='', openai_api_base=None, openai_api_type='', openai_proxy='', embedding_ctx_length=8191, openai_api_key=SecretStr('**********'), openai_organization=None, allowed_special=None, disallowed_special=None, chunk_size=1000, max_retries=2, request_timeout=None, headers=None, tiktoken_enabled=True, tiktoken_model_name=None, show_progress_bar=False, model_kwargs={}, skip_empty=False, default_headers=None, default_query=None, retry_min_seconds=4, retry_max_seconds=20, http_client=None, http_async_client=None, check_embedding_ctx_length=True),\n",
       " 'llm_provider': llm_analyst.chat_models.openai.OPENAI_Model,\n",
       " 'llm_model': 'gpt-4o-2024-05-13',\n",
       " 'llm_token_limit': 4000,\n",
       " 'llm_temperature': 0.35,\n",
       " 'browse_chunk_max_length': 8192,\n",
       " 'summary_token_limit': 700,\n",
       " 'max_search_results_per_query': 5,\n",
       " 'total_words': 1000,\n",
       " 'max_subsections': 5,\n",
       " 'max_iterations': 3,\n",
       " 'max_subtopics': 3,\n",
       " 'report_out_dir': '~/llm_analyst_out',\n",
       " 'local_store_dir': '/Users/dan/Code/LLM/amy_papers'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Let's run a simple research report against a set of local documemnts (Amy's Published Papers)\n",
    "## Three things are required.\n",
    "## 1. An active research topic \n",
    "## 2. A defined path to the local data to research against\n",
    "## 3. We must indicate the data source (LocalStore, Web, SelectURLs)\n",
    "\n",
    "# Requirement 1.\n",
    "research_topic = \"I would like to better understand how the metabolism of S-adenosylmethionine is linked to lipid metabolism and stress-responsive gene expression.\"\n",
    "\n",
    "# Requirement 2.\n",
    "config_params = {\n",
    "    \"internet_search\":\"ddg_search\",\n",
    "    \"llm_provider\"   :\"openai\",\n",
    "    \"llm_model\"      :\"gpt-4o-2024-05-13\",\n",
    "    \"local_store_dir\":\"/Users/dan/Code/LLM/amy_papers\"\n",
    "}\n",
    "config = Config()\n",
    "config._set_values_for_config(config_params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dan/miniforge3/envs/gpt-researcher/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report written to /Users/dan/llm_analyst_out/Research-2024-06-03-0959030520.pdf\n"
     ]
    }
   ],
   "source": [
    "# To execute some preliminary research we use the LLMAnalyst \n",
    "# request it conduct_research and then write_report\n",
    "llm_analyst = LLMAnalyst(active_research_topic = research_topic, \n",
    "                         data_source = DataSource.LocalStore, \n",
    "                         config = config)\n",
    "\n",
    "await llm_analyst.conduct_research()\n",
    "research_state = await llm_analyst.write_report()\n",
    "\n",
    "\n",
    "# Once the report is written we can ask the LLMPublisher to mak a pdf\n",
    "llm_publisher = LLMPublisher(**research_state.dump(), config = config)\n",
    "plublished_reseach_path = await llm_publisher.publish_to_pdf_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's build on the first research project and now build a \"detailed report\"\n",
    "## A \"detailed report\" requires oversite therefore we will use an Editor v.s. an Analyst\n",
    "## The key difference between an Editor and the Analyst is that\n",
    "## the Editor will coordinate the efforts of multiple Analysts and \n",
    "## will utilize a specilized Report Writer to pull the final report together\n",
    "\n",
    "## Inputs are the same as the Research Analyst Report above\n",
    "\n",
    "## But we will explicitly define the publish directory.\n",
    "## FYI this is the default so not actually required.\n",
    "config_params[\"report_out_dir\"] = \"~/llm_analyst_out\"\n",
    "\n",
    "llm_editor = LLMEditor(active_research_topic = research_topic, \n",
    "                       data_source = DataSource.LocalStore,\n",
    "                       config = config)\n",
    "\n",
    "research_state = await llm_editor.create_detailed_report()\n",
    "\n",
    "llm_publisher = LLMPublisher(**research_state.dump(), config = config)\n",
    "plublished_reseach_path = await llm_publisher.publish_to_pdf_file()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Get the current date and time\n",
    "now = datetime.now()\n",
    "\n",
    "# Format the date and time with milliseconds\n",
    "formatted_date_time = now.strftime('%Y-%m-%d-%H%M%S%f')[:-2]\n",
    "\n",
    "print(formatted_date_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2024-06-03-0950413600\n",
    "2024-06-03-0950561325"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt-researcher",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
