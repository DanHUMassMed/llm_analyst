{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This Notebook will demonstrate different configurations and executions of LLM-Analyst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Variables\n",
    "\n",
    "* For proper execution of LLM_ANalyst, one or more of the below environment variables may be required.\n",
    "* For these examples, we will require only `OPENAI_API_KEY`\n",
    "\n",
    "```bash\n",
    "export OPENAI_API_KEY=\"\"\n",
    "export TAVILY_API_KEY=\"\"\n",
    "export SERPER_API_KEY=\"\"\n",
    "export SERP_API_KEY=\"\"\n",
    "export HUGGINGFACEHUB_API_TOKEN=\"\"\n",
    "export LANGCHAIN_API_KEY=\"\"\n",
    "export GROQ_API_KEY=\"\"\n",
    "export GOOGLE_CX_KEY=\"\"\n",
    "export GOOGLE_API_KEY=\"\"\n",
    "export BING_API_KEY=\"\"\n",
    "export NCBI_API_KEY=\"\"\n",
    "export ORCID_ACCESS_TOKEN=\"\"\n",
    "export ORCID_REFRESH_TOKEN=\"\"\n",
    "export PYPI_API_TOKEN=\"\"\n",
    "export DOCKERHUB_API_TOKEN=\"\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites  \n",
    "* You will need a local Python environment with all the required Python packages installed.\n",
    "* If you are reading this, you have most likely already cloned the repo (If you have not)\n",
    "    * Execute:\n",
    "        ```bash\n",
    "        cd my_local_development_dir\n",
    "        git clone https://github.com/DanHUMassMed/llm_analyst.git\n",
    "        cd llm_analyst\n",
    "        ```\n",
    "* Create a python environment (We use Conda-Forge Miniforge3)\n",
    "    * Execute:\n",
    "        ```bash\n",
    "        conda create -n llm-analyst python=3.11 ipykernel\n",
    "        conda activate llm-analyst\n",
    "        pip install -r requirements.txt\n",
    "        ```\n",
    "\n",
    "-----\n",
    "\n",
    "NOTE: You could also just execute `pip install research-task`\n",
    "\n",
    "However, we expect you want to play with the code, not just use the package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System level imports\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# ##### SET SYS PATH TO WHERE THE CODE IS. #####\n",
    "# my_local_development_dir/llm_analyst\n",
    "# Note: Putting our code first in the sys path will make sure it gets picked up\n",
    "llm_analyst_base_dir='/Users/dan/Code/LLM/llm_analyst'\n",
    "sys.path.insert(0, llm_analyst_base_dir)\n",
    "\n",
    "\n",
    "# Setting the USER_AGENT to fix warning with langchain_community code\n",
    "# WARNING:langchain_community.utils.user_agent:USER_AGENT\n",
    "user_agent = (\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n",
    "              \"(KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36 Edg/119.0.0.0\")\n",
    "os.environ['USER_AGENT'] = user_agent\n",
    "\n",
    "# Setting the OPENAI_API_KEY\n",
    "#os.environ['OPENAI_API_KEY'] = \"*****************************88\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:pyvirtualdisplay:version=3.0\n"
     ]
    }
   ],
   "source": [
    "# Let's import llm_analyst content in one cell to make the rest of the code a little cleaner.\n",
    "from llm_analyst.core.config import Config, DataSource\n",
    "from llm_analyst.core.research_analyst import LLMAnalyst\n",
    "from llm_analyst.core.research_editor import LLMEditor\n",
    "from llm_analyst.core.research_publisher import LLMPublisher\n",
    "from llm_analyst.core.config import Config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstrate running LLM-Analyst on Local Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get some realistic data to run our reports against\n",
    "# We will download a set of papers from the Walker Lab\n",
    "\n",
    "walker_lab_papers = f\"{llm_analyst_base_dir}/reports_data/Walker_Lab_Papers\"\n",
    " \n",
    "!git clone https://github.com/DanHUMassMed/Walker_Lab_Papers.git {walker_lab_papers}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now let's run a simple research report against a set of local documents (Published Papers)\n",
    "## Three things are required.\n",
    "## 1. An active research topic \n",
    "## 2. A defined path to the local data to research against\n",
    "## 3. Indicate the data source (LOCAL_STORE, WEB, SELECT_URLS)\n",
    "\n",
    "# Requirement 1 (research topic).\n",
    "# Oh cool researching metabolism!\n",
    "research_topic = \"I would like to better understand how the metabolism of S-adenosylmethionine is linked to lipid metabolism and stress-responsive gene expression.\"\n",
    "\n",
    "# Requirement 2 (local data to research).\n",
    "# We add a few additional config_params to just be explicit. \n",
    "# NOTE: The defaults would also work fine. \n",
    "config_params = {\n",
    "    \"internet_search\" :\"ddg_search\",\n",
    "    \"llm_provider\"    :\"openai\",\n",
    "    \"llm_model\"       :\"gpt-4o-2024-05-13\",\n",
    "    \"local_store_dir\" :f\"{walker_lab_papers}\",\n",
    "    \"report_out_dir\"  :f\"{llm_analyst_base_dir}/notebooks/data\",\n",
    "    \"cache_dir\"       :f\"{llm_analyst_base_dir}/notebooks/data/cache\"\n",
    "}\n",
    "config = Config()\n",
    "config.set_values_for_config(config_params)\n",
    "print(config)\n",
    "\n",
    "# Take note of the config values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have set things up, let's get down to conducting the research!\n",
    "# To execute preliminary research, we use the LLMAnalyst Object\n",
    "# Request the analyst to conduct research and then writes a report\n",
    "\n",
    "llm_analyst = LLMAnalyst(active_research_topic = research_topic, \n",
    "                         data_source = DataSource.LOCAL_STORE, \n",
    "                         config = config)\n",
    "\n",
    "await llm_analyst.conduct_research()\n",
    "research_state = await llm_analyst.write_report()\n",
    "\n",
    "\n",
    "# Once the report is written, we can ask the LLMPublisher to make a pdf\n",
    "llm_publisher = LLMPublisher(**research_state.dump(), config = config)\n",
    "published_research_path = await llm_publisher.publish_to_pdf_file()\n",
    "published_research_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Very cool we just created a preliminary research report!\n",
    "## Now we have decided that based on the collected data, we want to see a \"detailed report\"\n",
    "## \n",
    "## A \"detailed report\" requires oversight, therefore, we will use an LLMEditor v.s. an LLMAnalyst\n",
    "## The key difference between an Editor and the Analyst is that\n",
    "## the Editor will coordinate the efforts of multiple Analysts and \n",
    "## will utilize a specialized Report Writer to pull the final report together\n",
    "\n",
    "## The inputs are the same as the Research Analyst Report above\n",
    "\n",
    "llm_editor = LLMEditor(active_research_topic = research_topic, \n",
    "                       data_source = DataSource.LOCAL_STORE,\n",
    "                       config = config)\n",
    "\n",
    "research_state = await llm_editor.create_detailed_report()\n",
    "\n",
    "llm_publisher = LLMPublisher(**research_state.dump(), config = config)\n",
    "published_research_path = await llm_publisher.publish_to_pdf_file()\n",
    "\n",
    "# NOTE As the Code runs, you should see the logging line below\n",
    "# INFO:root:*** Using Cached Repo. ***\n",
    "# This is indicating that we are not recreating the embedding\n",
    "# Instead we are using the cached data in the vector db\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstrate Running LLM-Analyst on Web Scraped Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's run a simple research report against the internet\n",
    "## All that is required is an active research topic\n",
    "\n",
    "# Requirement 1.\n",
    "research_topic = \"How does DAF-19 regulate transcription of regeneration associated genes?\"\n",
    "\n",
    "# \n",
    "config = Config()\n",
    "print(config)\n",
    "\n",
    "# Take note of the default config values\n",
    "# Pay particular attention to the report_out_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To execute some preliminary research, we use the LLMAnalyst \n",
    "\n",
    "# Note we are using the defaults data_source and config so we do not need to provide them\n",
    "llm_analyst = LLMAnalyst(active_research_topic = research_topic)\n",
    "\n",
    "await llm_analyst.conduct_research()\n",
    "research_state = await llm_analyst.write_report()\n",
    "\n",
    "# Once the report is written, we can ask the LLMPublisher to make a pdf\n",
    "llm_publisher = LLMPublisher(**research_state.dump(), config = config)\n",
    "published_research_path = await llm_publisher.publish_to_pdf_file()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's build on the first internet research project and now build a \"detailed report\"\n",
    "\n",
    "## Inputs are the same as the Research Report above\n",
    "\n",
    "llm_editor = LLMEditor(active_research_topic = research_topic)\n",
    "\n",
    "research_state = await llm_editor.create_detailed_report()\n",
    "\n",
    "llm_publisher = LLMPublisher(**research_state.dump(), config = config)\n",
    "published_research_path = await llm_publisher.publish_to_pdf_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt-researcher",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
