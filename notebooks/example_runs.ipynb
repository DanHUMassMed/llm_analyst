{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This Notebook will demonstrate different configurations and executions of LLM-Analyst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Variables\n",
    "\n",
    "* For proper execution one or more of the below environment variables may be required.\n",
    "* For these examples we will require `OPENAI_API_KEY`\n",
    "\n",
    "```bash\n",
    "export OPENAI_API_KEY=\"\"\n",
    "export TAVILY_API_KEY=\"\"\n",
    "export SERPER_API_KEY=\"\"\n",
    "export SERP_API_KEY=\"\"\n",
    "export HUGGINGFACEHUB_API_TOKEN=\"\"\n",
    "export LANGCHAIN_API_KEY=\"\"\n",
    "export GROQ_API_KEY=\"\"\n",
    "export GOOGLE_CX_KEY=\"\"\n",
    "export GOOGLE_API_KEY=\"\"\n",
    "export BING_API_KEY=\"\"\n",
    "export NCBI_API_KEY=\"\"\n",
    "export ORCID_ACCESS_TOKEN=\"\"\n",
    "export ORCID_REFRESH_TOKEN=\"\"\n",
    "export PYPI_API_TOKEN=\"\"\n",
    "export DOCKERHUB_API_TOKEN=\"\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System level imports\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Setting the USER_AGENT to fix warning with langchain_community code\n",
    "# WARNING:langchain_community.utils.user_agent:USER_AGENT\n",
    "user_agent = (\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n",
    "              \"(KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36 Edg/119.0.0.0\")\n",
    "os.environ['USER_AGENT'] = user_agent\n",
    "\n",
    "# PATH TO WHERE THE CODE WAS \"GIT CLONED\"\n",
    "llm_analyst_base_dir='/Users/dan/Code/LLM/llm_analyst'\n",
    "sys.path.insert(0, llm_analyst_base_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:pyvirtualdisplay:version=3.0\n"
     ]
    }
   ],
   "source": [
    "# Let's import llm_analyst content in one cell to make the rest of the code cleaner.\n",
    "from llm_analyst.core.config import Config, DataSource\n",
    "from llm_analyst.core.research_analyst import LLMAnalyst\n",
    "from llm_analyst.core.research_editor import LLMEditor\n",
    "from llm_analyst.core.research_publisher import LLMPublisher\n",
    "from llm_analyst.core.config import Config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstrate running LLM-Analyst on Local Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "internet_search=<function ddg_search at 0x10cb15260>\n",
      "embedding_provider=client=<openai.resources.embeddings.Embeddings object at 0x168be2c10> async_client=<openai.resources.embeddings.AsyncEmbeddings object at 0x168d39550> model='text-embedding-ada-002' dimensions=None deployment='text-embedding-ada-002' openai_api_version='' openai_api_base=None openai_api_type='' openai_proxy='' embedding_ctx_length=8191 openai_api_key=SecretStr('**********') openai_organization=None allowed_special=None disallowed_special=None chunk_size=1000 max_retries=2 request_timeout=None headers=None tiktoken_enabled=True tiktoken_model_name=None show_progress_bar=False model_kwargs={} skip_empty=False default_headers=None default_query=None retry_min_seconds=4 retry_max_seconds=20 http_client=None http_async_client=None check_embedding_ctx_length=True\n",
      "llm_provider=<class 'llm_analyst.chat_models.openai.OPENAI_Model'>\n",
      "llm_model=gpt-4o-2024-05-13\n",
      "llm_token_limit=4000\n",
      "llm_temperature=0.25\n",
      "browse_chunk_max_length=8192\n",
      "summary_token_limit=700\n",
      "max_search_results_per_query=5\n",
      "total_words=1000\n",
      "max_subsections=5\n",
      "max_iterations=3\n",
      "max_subtopics=3\n",
      "report_out_dir=/Users/dan/Code/LLM/llm_analyst/notebooks/data\n",
      "local_store_dir=/Users/dan/Code/LLM/llm_analyst/tests/resources/tst_documents\n",
      "cache_dir=/Users/dan/.cache/llm_analyst\n"
     ]
    }
   ],
   "source": [
    "## Let's run a simple research report against a set of local documents (Published Papers)\n",
    "## Three things are required.\n",
    "## 1. An active research topic \n",
    "## 2. A defined path to the local data to research against\n",
    "## 3. Indicate the data source (LOCAL_STORE, WEB, SELECT_URLS)\n",
    "\n",
    "# Requirement 1.\n",
    "research_topic = \"I would like to better understand how the metabolism of S-adenosylmethionine is linked to lipid metabolism and stress-responsive gene expression.\"\n",
    "\n",
    "# Requirement 2.\n",
    "config_params = {\n",
    "    \"internet_search\" :\"ddg_search\",\n",
    "    \"llm_provider\"    :\"openai\",\n",
    "    \"llm_model\"       :\"gpt-4o-2024-05-13\",\n",
    "    \"local_store_dir\" :f\"{llm_analyst_base_dir}/tests/resources/tst_documents\",\n",
    "    \"report_out_dir\"  :f\"{llm_analyst_base_dir}/notebooks/data\"\n",
    "}\n",
    "config = Config()\n",
    "config.set_values_for_config(config_params)\n",
    "print(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we has set things up lets get down to conducting the research!\n",
    "# To execute preliminary research we use the LLMAnalyst \n",
    "# Request the analysts conducts research and then writes a report\n",
    "\n",
    "llm_analyst = LLMAnalyst(active_research_topic = research_topic, \n",
    "                         data_source = DataSource.LOCAL_STORE, \n",
    "                         config = config)\n",
    "\n",
    "await llm_analyst.conduct_research()\n",
    "research_state = await llm_analyst.write_report()\n",
    "\n",
    "\n",
    "# Once the report is written we can ask the LLMPublisher to make a pdf\n",
    "llm_publisher = LLMPublisher(**research_state.dump(), config = config)\n",
    "published_research_path = await llm_publisher.publish_to_pdf_file()\n",
    "published_research_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's build on the first research project and now build a \"detailed report\"\n",
    "## A \"detailed report\" requires over site therefore we will use an Editor v.s. an Analyst\n",
    "## The key difference between an Editor and the Analyst is that\n",
    "## the Editor will coordinate the efforts of multiple Analysts and \n",
    "## will utilize a specialized Report Writer to pull the final report together\n",
    "\n",
    "## Inputs are the same as the Research Analyst Report above\n",
    "\n",
    "## But we will explicitly define the publish directory.\n",
    "## FYI this is the default so not actually required.\n",
    "config_params[\"report_out_dir\"] = \"~/llm_analyst_out\"\n",
    "\n",
    "llm_editor = LLMEditor(active_research_topic = research_topic, \n",
    "                       data_source = DataSource.LOCAL_STORE,\n",
    "                       config = config)\n",
    "\n",
    "research_state = await llm_editor.create_detailed_report()\n",
    "\n",
    "llm_publisher = LLMPublisher(**research_state.dump(), config = config)\n",
    "published_research_path = await llm_publisher.publish_to_pdf_file()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstrate Running LLM-Analyst on Web Scraped Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's run a simple research report against the internet\n",
    "## All that is required is an active research topic\n",
    "\n",
    "# Requirement 1.\n",
    "research_topic = \"How does DAF-19 regulate transcription of regeneration associated genes?\"\n",
    "\n",
    "\n",
    "config_params = {\n",
    "    \"internet_search\":\"ddg_search\",\n",
    "    \"llm_provider\"   :\"openai\",\n",
    "    \"llm_model\"      :\"gpt-4o-2024-05-13\",\n",
    "}\n",
    "config = Config()\n",
    "config.set_values_for_config(config_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To execute some preliminary research we use the LLMAnalyst \n",
    "\n",
    "llm_analyst = LLMAnalyst(active_research_topic = research_topic, \n",
    "                         config = config)\n",
    "\n",
    "await llm_analyst.conduct_research()\n",
    "research_state = await llm_analyst.write_report()\n",
    "\n",
    "\n",
    "# Once the report is written we can ask the LLMPublisher to mak a pdf\n",
    "llm_publisher = LLMPublisher(**research_state.dump(), config = config)\n",
    "published_research_path = await llm_publisher.publish_to_pdf_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's build on the first research project and now build a \"detailed report\"\n",
    "## A \"detailed report\" requires oversite therefore we will use an Editor v.s. an Analyst\n",
    "## The key difference between an Editor and the Analyst is that\n",
    "## the Editor will coordinate the efforts of multiple Analysts and \n",
    "## will utilize a specilized Report Writer to pull the final report together\n",
    "\n",
    "## Inputs are the same as the Research Analyst Report above\n",
    "\n",
    "## But we will explicitly define the publish directory.\n",
    "## FYI this is the default so not actually required.\n",
    "config_params[\"report_out_dir\"] = \"~/llm_analyst_out\"\n",
    "\n",
    "llm_editor = LLMEditor(active_research_topic = research_topic, \n",
    "                       data_source = DataSource.LOCAL_STORE,\n",
    "                       config = config)\n",
    "\n",
    "research_state = await llm_editor.create_detailed_report()\n",
    "\n",
    "llm_publisher = LLMPublisher(**research_state.dump(), config = config)\n",
    "published_research_path = await llm_publisher.publish_to_pdf_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt-researcher",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
