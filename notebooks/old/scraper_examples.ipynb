{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/dan/Code/LLM/llm_researcher')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.retrievers import ArxivRetriever\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_community.document_loaders.html_bs import BSHTMLLoader\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from bs4 import BeautifulSoup\n",
    "import uuid\n",
    "import requests\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Large language models (LLMs) have recently been shown to deliver impressive\\nperformance in various NLP tasks. To tackle multi-step reasoning tasks,\\nfew-shot chain-of-thought (CoT) prompting includes a few manually crafted\\nstep-by-step reasoning demonstrations which enable LLMs to explicitly generate\\nreasoning steps and improve their reasoning task accuracy. To eliminate the\\nmanual effort, Zero-shot-CoT concatenates the target problem statement with\\n\"Let\\'s think step by step\" as an input prompt to LLMs. Despite the success of\\nZero-shot-CoT, it still suffers from three pitfalls: calculation errors,\\nmissing-step errors, and semantic misunderstanding errors. To address the\\nmissing-step errors, we propose Plan-and-Solve (PS) Prompting. It consists of\\ntwo components: first, devising a plan to divide the entire task into smaller\\nsubtasks, and then carrying out the subtasks according to the plan. To address\\nthe calculation errors and improve the quality of generated reasoning steps, we\\nextend PS prompting with more detailed instructions and derive PS+ prompting.\\nWe evaluate our proposed prompting strategy on ten datasets across three\\nreasoning problems. The experimental results over GPT-3 show that our proposed\\nzero-shot prompting consistently outperforms Zero-shot-CoT across all datasets\\nby a large margin, is comparable to or exceeds Zero-shot-Program-of-Thought\\nPrompting, and has comparable performance with 8-shot CoT prompting on the math\\nreasoning problem. The code can be found at\\nhttps://github.com/AGI-Edgerunners/Plan-and-Solve-Prompting.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "link=\"https://arxiv.org/abs/2305.04091\"\n",
    "def arxiv_scrape(link):\n",
    "    query = link.split(\"/\")[-1]\n",
    "    retriever = ArxivRetriever(load_max_docs=2, doc_content_chars_max=None)\n",
    "    docs = retriever.invoke(query)\n",
    "    # Just pulling the abstract\n",
    "    return docs[0].page_content\n",
    "\n",
    "arxiv_scrape(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought\\nReasoning by Large Language Models\\nLei Wang1 Wanyu Xu2 Yihuai Lan Zhiqiang Hu3 Yunshi Lan4\\nRoy Ka-Wei Lee3 Ee-Peng Lim1∗\\n1Singapore Management University\\n2Southwest Jiaotong University\\n3Singapore University of Technology and Design\\n4East China Normal University\\nAbstract\\nLarge language models (LLMs) have recently\\nbeen shown to deliver impressive performance\\nin various NLP tasks. To tackle multi-step rea-\\nsoning tasks, few-shot chain-of-thought (CoT)\\nprompting includes a few manually crafted\\nstep-by-step reasoning demonstrations which\\nenable LLMs to explicitly generate reasoning\\nsteps and improve their reasoning task accu-\\nracy. To eliminate the manual effort, Zero-\\nshot-CoT concatenates the target problem state-\\nment with “Let’s think step by step” as an in-\\nput prompt to LLMs. Despite the success of\\nZero-shot-CoT, it still suffers from three pit-\\nfalls: calculation errors, missing-step errors,\\nand semantic misunderstanding errors. To ad-\\ndress the missing-step errors, we propose Plan-\\nand-Solve (PS) Prompting. It consists of two\\ncomponents: first, devising a plan to divide the\\nentire task into smaller subtasks, and then car-\\nrying out the subtasks according to the plan.\\nTo address the calculation errors and improve\\nthe quality of generated reasoning steps, we\\nextend PS prompting with more detailed in-\\nstructions and derive PS+ prompting. We eval-\\nuate our proposed prompting strategy on ten\\ndatasets across three reasoning problems. The\\nexperimental results over GPT-3 show that our\\nproposed zero-shot prompting consistently out-\\nperforms Zero-shot-CoT across all datasets by\\na large margin, is comparable to or exceeds\\nZero-shot-Program-of-Thought Prompting, and\\nhas comparable performance with 8-shot CoT\\nprompting on the math reasoning problem. The\\ncode can be found at https://github.com/AGI-\\nEdgerunners/Plan-and-Solve-Prompting.\\n1\\nIntroduction\\nLarge language models (LLMs) (Brown et al.,\\n2020; Thoppilan et al., 2022; Chowdhery et al.,\\n2022) have recently proven highly effective in var-\\nious NLP tasks. Unlike the previous pre-trained\\nlanguage models (PTMs) (Devlin et al., 2019; Liu\\n∗Corresponding author.\\n5\\n10\\n15\\n20\\n25\\n30\\n35\\nRatio (%)\\n7\\n12\\n27\\nCalculation Error\\nStep Missing Error\\nSemantic Misunderstanding\\nFigure 1: Error analysis of 46 GSM8K problems with in-\\ncorrect answers returned by Zero-shot-CoT using GPT-\\n3 LLM. Following Wei et al. (2022b) and Wang et al.\\n(2022a), we assign “Calculation Error” (7%), “Step\\nMissing Error” (12%), or “Semantic misunderstanding\\nError” (27%) to each incorrect answer.\\net al., 2019), these LLMs are typically provided as\\na service, with no access to model parameters due\\nto commercial considerations and potential risks of\\nmisuse (Sun et al., 2022). Thus, it is challenging\\nto fine-tune LLMs for downstream tasks (He et al.,\\n2021; Houlsby et al., 2019; Devlin et al., 2019).\\nInstead, we leverage LLMs to solve complex rea-\\nsoning problems by eliciting their strong reasoning\\nabilities over their embedded knowledge using in-\\nstructions (or trigger sentences). So far, LLMs have\\nshown impressive abilities to solve new reasoning\\nproblems by simply conditioning them on a few\\nillustrative examples (i.e., few-shot learning) or a\\nprompt to solve new problems without illustrative\\nexamples (i.e., zero-shot learning).\\nTo tackle multi-step complex reasoning tasks us-\\ning LLMs, Wei et al. (2022b) proposes few-shot\\nchain-of-thought (CoT) prompting, which enables\\nLLMs to explicitly generate the intermediate rea-\\nsoning steps before predicting the final answer with\\na few manual step-by-step reasoning demonstra-\\ntion examples. In (Kojima et al., 2022), Zero-shot\\nCoT eliminates the need for manually crafted ex-\\namples in prompts by appending “Let’s think step\\nby step” to the target problem fed to LLMs such\\narXiv:2305.04091v3  [cs.CL]  26 May 2023\\nas GPT-3. This simple prompting strategy surpris-\\ningly enables LLMs to yield performance similar\\nto few-shot CoT prompting.\\nDespite the remarkable success of Zero-shot-\\nCoT in solving multi-step reasoning tasks, its re-\\nsults on a sample of 100 arithmetic test examples\\nstill point to three pitfalls (as shown in Figure 1):\\n(i) Calculation errors (in 7% of test examples):\\nThese are errors in the calculation leading to wrong\\nanswers; (ii) Missing Step errors (in 12% of test\\nexamples): These occur when some intermediate\\nreasoning step(s) is missed-out especially when\\nthere are many steps involved; (iii) Semantic mis-\\nunderstanding (in 27% of test examples): There\\nare other errors in semantic understanding of the\\nproblem and coherence of reasoning steps likely to\\nbe caused by the insufficient capability of LLMs.\\nTo address the issue of Zero-shot-CoT caused\\nby missing reasoning steps, we propose Plan-and-\\nSolve (PS) Prompting. It consists of two compo-\\nnents: first, devising a plan to divide the entire task\\ninto smaller subtasks, and then carrying out the sub-\\ntasks according to the plan. In our experiments, we\\nsimply replace “Let’s think step by step” of Zero-\\nshot-CoT with “Let’s first understand the problem\\nand devise a plan to solve the problem. Then, let’s\\ncarry out the plan and solve the problem step by\\nstep” (see Figure 2 (b)).\\nTo address the calculation errors of Zero-shot-\\nCoT and improve the quality of generated reason-\\ning steps, we add more detailed instructions to PS\\nprompting. Specifically, we extend it with “extract\\nrelevant variables and their corresponding numer-\\nals” and “calculate intermediate results (pay atten-\\ntion to calculation and commonsense)” instructions.\\nThis prompting variant is called the PS+ prompting\\nstrategy (see Figure 3 (b)). Despite its simplic-\\nity, PS+ strategy greatly improves the quality of\\nthe generated reasoning process. Moreover, this\\nprompting strategy can be easily customized to\\nsolve a variety of problems other than math reason-\\ning, such as commonsense and symbolic reasoning\\nproblems.\\nWe evaluate our proposed prompting on six\\nmath reasoning datasets, including AQuA (Ling\\net al., 2017), GSM8K (Cobbe et al., 2021), Mul-\\ntiArith, AddSub, SingleEq, and SVAMP (Patel\\net al., 2021), two commonsense reasoning datasets\\n(CommonsenseQA (Talmor et al., 2019) and Strat-\\negyQA (Geva et al., 2021)), and two symbolic rea-\\nsoning datasets (Last Letter and Coin Flip (Wei\\net al., 2022b)). The results of our experiments\\nwith GPT-3 show that our proposed Zero-shot-PS+\\nprompting consistently outperforms Zero-shot-CoT\\nacross all reasoning problems and datasets by a\\nlarge margin, and is comparable to or exceeds Zero-\\nshot-Program-of-Thought (PoT) Prompting (Chen\\net al., 2022)). Furthermore, although PS+ prompt-\\ning does not require manual demonstration exam-\\nples, it has a performance similar to an 8-shot CoT\\nprompting in arithmetic reasoning.\\nOverall, our results suggest that (a) Zero-shot PS\\nprompting is capable of generating a higher-quality\\nreasoning process than Zero-shot-CoT prompting,\\nas the PS prompts provide more detailed instruc-\\ntions guiding the LLMs to perform correct rea-\\nsoning tasks; (b) Zero-shot PS+ prompting outper-\\nforms Few-shot manual-CoT prompting on some\\ndatasets, indicating that in some instances it has\\nthe potential to outperform manual Few-shot CoT\\nprompting, which hopefully will spark further de-\\nvelopment of new CoT prompting approaches to\\nelicit reasoning in LLMs.\\n2\\nPlan-and-Solve Prompting\\nOverview.\\nWe introduce PS prompting, a new\\nzero-shot CoT prompting method, which enables\\nLLMs to explicitly devise a plan for solving a given\\nproblem and generate the intermediate reasoning\\nprocess before predicting the final answer for the\\ninput problem. As opposed to prior few-shot CoT\\napproaches where step-by-step few-shot demon-\\nstration examples are included in the prompt, the\\nzero-shot PS prompting method does not require\\ndemonstration examples, and its prompt covers\\nthe problem itself and a simple trigger sentence.\\nSimilar to Zero-shot-CoT, Zero-shot PS prompting\\nconsists of two steps. In step 1, the prompt first\\nmakes an inference using the proposed prompting\\ntemplate to generate the reasoning process and the\\nanswer to a problem. In step 2, it extracts the an-\\nswer for evaluation by using the answer extraction\\nprompting, such as “Therefore, the answer (arabic\\nnumerals) is”.\\n2.1\\nStep 1: Prompting for Reasoning\\nGeneration\\nTo solve the input problem while avoiding errors\\nresulting from incorrect calculation and missing\\nreasoning steps, this step aims to construct tem-\\nplates to meet the following two criteria:\\n• The templates should elicit LLMs to deter-\\nFigure 2: Example inputs and outputs of GPT-3 with (a) Zero-shot-CoT prompting, (b) Plan-and-Solve (PS)\\nprompting, and (c) answer extraction prompting. While Zero-shot-CoT encourages LLMs to generate multi-step\\nreasoning with “Let’s think step by step”, it may still generate wrong reasoning steps when the problem is complex.\\nUnlike Zero-shot-CoT, PS prompting first asks LLMs to devise a plan to solve the problem by generating a step-by-\\nstep plan and carrying out the plan to find the answer.\\nmine subtasks and accomplish the subtasks.\\n• The templates should guide LLMs to pay\\nmore attention to calculations and intermedi-\\nate results and to ensure that they are correctly\\nperformed as much as possible.\\nTo meet the first criterion, we follow Zero-shot-\\nCoT and first convert the input data example into\\na prompt with a simple template “Q: [X]. A:\\n[T]”. Specifically, the input slot [X] contains\\nthe input problem statement and a hand-crafted\\ninstruction is specified in the input slot [T] to\\ntrigger LLMs to generate a reasoning process that\\nincludes a plan and steps to complete the plan.\\nIn Zero-shot-CoT, the instruction in the input\\nslot [T] includes the trigger instruction ‘Let’s\\nthink step by step”. Our Zero-shot PS prompting\\nmethod instead includes the instructions “devise\\na plan” and “carry out the plan” as shown in\\nFigure 2(b).\\nThus, the prompt would be “Q:\\n[X]. A: Let’s first understand the problem and\\ndevise a plan to solve the problem.\\nThen, let’s\\ncarry out the plan and solve the problem step by\\nstep.”\\nWe then pass the above prompt to the LLM\\nwhich subsequently outputs a reasoning process. In\\naccordance with Zero-shot-CoT, our method uses\\nthe greedy decoding strategy (1 output chain) for\\ngenerating output by default.\\nTo meet the second criterion, we extend the plan-\\nbased trigger sentence with more detailed instruc-\\ntions. Specifically, “pay attention to calculation” is\\nadded to the trigger sentence to request the LLMs to\\nperform calculations as accurately as possible. To\\nreduce errors resulting from missing necessary rea-\\nsoning steps, we include “extract relevant variables\\nand their corresponding numerals” to explicitly in-\\nstruct the LLMs not to ignore relevant information\\nin the input problem statement. We hypothesize\\nthat if the LLMs leave out the relevant and impor-\\ntant variables, it is more likely to miss out relevant\\nreasoning steps. Correlation analysis of generated\\ncontent of variable and the missing reasoning step\\nerrors, shown in Figure 5, empirically supports this\\nhypothesis (correlation value is less than 0). Ad-\\nditionally, we add “calculate intermediate results”\\nto the prompt to enhance LLM’s ability to gener-\\nate relevant and important reasoning steps. The\\nspecific example is illustrated in Figure 3(b). At\\nthe end of Step 1, LLM generates the reasoning\\ntext which includes the answer. For example, the\\ngenerated reasoning text in Figure 3(b) includes\\n“Combined weight of Grace and Alex = 125 + 498\\n= 623 pounds”. The strategy of adding specific de-\\nscriptions to the trigger sentence represents a new\\nway to improve zero-shot performance on complex\\nreasoning.\\n2.2\\nStep 2: Prompting for Answer Extraction\\nSimilar to Zero-shot-CoT, we devise another\\nprompt in Step 2 to get the LLM to extract the final\\nnumerical answer from the reasoning text gener-\\nFigure 3: Example inputs and outputs of GPT-3 with (a) Plan-and-Solve (PS) Prompting and (b) Plan-and-Solve\\nprompting with more detailed instructions (PS+ prompting). PS+ prompting greatly improves the quality of the\\ngenerated reasoning process.\\nated in Step 1. This prompt includes the answer ex-\\ntraction instruction appended to the first prompt fol-\\nlowed by the LLM generated reasoning text. This\\nway, LLM is expected to return the final answer in\\nthe desired form.\\nBased on the example in Figure 3(b), the\\nprompt used in Step 2 will include “Q: Grace\\nweighs 125 pounds\\n· · · Variables:\\nGrace:\\n125 pounds · · · Answer:\\nCombined weight of\\nGrace and Alex = 125 + 498 = 623 pounds.\\nTherefore, the answer (arabic numerals) is”.\\nFor\\nthis example, the final answer returned by LLM\\nis “623”.\\n3\\nExperimental Setup\\n3.1\\nBenchmarks\\nThe proposed method is evaluated on the ten bench-\\nmark datasets from three categories of reason-\\ning problems: Arithmetic Reasoning: (1) the\\nGSM8K (Cobbe et al., 2021) dataset of high qual-\\nity linguistically diverse grade school math word\\nproblems created by human problem writers, (2)\\nthe SVAMP (Patel et al., 2021) benchmark of one-\\nunknown arithmetic word problems for up-to-4\\ngrade level students by making simple changes\\nto a set of problems from another existing dataset,\\n(3) the MultiArith (Roy and Roth, 2016) dataset\\nof math word problems requiring multiple reason-\\ning steps and operations, (4) the AddSub (Hosseini\\net al., 2014) dataset of addition and subtraction\\narithmetic word problems, (5) the AQUA (Ling\\net al., 2017) dataset of algebraic word problems\\nwith natural language rationales, and (6) the Sin-\\ngleEq (Koncel-Kedziorski et al., 2015) dataset of\\nsingle-equation grade-school algebra word prob-\\nlems with multiple math operations over non-\\nnegative rational numbers and one variable; Com-\\nmonsense Reasoning: (7) the CSQA (Talmor et al.,\\n2019) benchmark dataset of multiple-choice ques-\\ntions that require different types of commonsense\\nknowledge to obtain the correct answers; and (8)\\nthe StrategyQA (Geva et al., 2021) benchmark\\ndataset with questions requiring multi-step reason-\\ning but the reasoning steps are not given. Hence,\\nthey are to be inferred; Symbolic Reasoning: (9)\\nthe Last Letter Concatenation (Wei et al., 2022b)\\ndataset of questions requiring the last letters of\\nwords in a name to be concatenated (e.g., “James\\nBrown” →“sn”), and (10) the Coin Flip (Wei et al.,\\n2022b) dataset of questions on whether a coin is\\nstill heads up after it is flipped or not flipped based\\non steps given in the questions. Table 1 shows\\ndataset statistics.\\nTable 1: Details of datasets being evaluated. Math: arith-\\nmetic reasoning. CS: commonsense reasoning. Sym.:\\nsymbolic reasoning.\\nDataset\\nDomain # Samples Ave. words\\nAnswer\\nMultiArith\\nMath\\n600\\n31.8\\nNumber\\nAddSub\\nMath\\n395\\n31.5\\nNumber\\nGSM8K\\nMath\\n1319\\n46.9\\nNumber\\nAQUA\\nMath\\n254\\n51.9\\nOption\\nSingleEq\\nMath\\n508\\n27.4\\nNumber\\nSVAMP\\nMath\\n1000\\n31.8\\nNumber\\nCSQA\\nCS\\n1221\\n27.8\\nOption\\nStrategyQA\\nCS\\n2290\\n9.6\\nYes / No\\nLast Letters\\nSym.\\n500\\n15.0\\nString\\nCoin Flip\\nSym.\\n500\\n37.0\\nYes / No\\n3.2\\nZero-shot and Few-shot Baselines\\nWe compare our proposed zero-shot PS and PS+\\nprompting methods with three types of prompt-\\ning baselines: (1) Zero-shot baselines. We in-\\nclude zero-shot-CoT (Kojima et al., 2022) and zero-\\nshot-PoT (Chen et al., 2022). The former appends\\n“Let’s think step by step” to the prompt without\\nany demonstration examples. The latter uses LLM\\n(mainly OpenAI Codex1) to generate a Python pro-\\ngram and then derive an answer by executing the\\ngenerated program on a Python interpreter; (2)\\nFew-shot with manual demonstrations. Manual-\\nCoT (Wei et al., 2022b) creates eight hand-crafted\\nexamples as demonstrations. (3) Few-shot with au-\\ntomatic demonstrations. Auto-CoT (Zhang et al.,\\n2022) automatically selected examples by cluster-\\ning with diversity and generates reasoning chains\\nusing zero-shot-CoT to construct demonstrations.\\n3.3\\nImplementations\\nFollowing Auto-CoT (Zhang et al., 2022), we use\\nthe public GPT-3 (Brown et al., 2020) (175B) as\\nthe backbone language model, which is one of\\nthe most widely-used LLMs with public APIs2.\\nSince text-davinci-003 is an upgraded ver-\\nsion of text-davinci-002, which can pro-\\nduce higher-quality writing, accommodate more\\ncomplex instructions, and perform better at longer-\\nform content generation, We report the results\\nusing text-davinci-003 engine for GPT-3\\nin the main paper. We set the temperature to 0\\n(argmax sampling) throughout our experiments for\\nthe greedy decoding strategy. We also include two\\nfew-shot baselines, Manual-CoT and Auto-CoT,\\nwe use 8 demonstration examples for MultiArith,\\nGSM8K, AddSub, SingleEq, and SVAMP, 4 ex-\\n1https://openai.com/blog/openai-codex/\\n2https://beta.openai.com/docs/models/gpt-3\\namples for AQuA and Last Letters, 7 examples\\nfor CSQA, and 6 examples for StrategyQA as sug-\\ngested in the original papers, Wei et al. (2022b) and\\nZhang et al. (2022). Evaluation metrics wise, we\\nfollow Manual-CoT (Wei et al., 2022b) and report\\nthe accuracy of all methods across datasets.\\n4\\nExperimental Results\\n4.1\\nMain Results\\nArithmetic Reasoning.\\nTable 2 reports the accu-\\nracy comparison of our method and existing zero-\\nshot and few-shot methods on the arithmetic rea-\\nsoning datasets. In the zero-shot setting, our PS+\\nprompting (i.e., PS prompting with more detailed\\ninstructions) consistently outperforms Zero-shot-\\nCoT across all arithmetic reasoning datasets by\\na large margin. Specifically, PS+ prompting im-\\nproves the accuracy over Zero-shot CoT by at least\\n5% for all datasets except GSM8K which sees a\\n2.9% improvement. The exception could be due to\\nGSM8K being a more challenging dataset from the\\nlinguistics complexity aspect. PS prompting also\\noutperforms Zero-shot-CoT across all datasets, and\\nenjoys 2.5% higher average accuracy than that of\\nZero-shot CoT.\\nCompared with another competitive Zero-shot\\nbaseline, PoT, the performance of PS(+) and PS\\npromptings are still impressive. PS+ prompting out-\\nperforms PoT on five out of six arithmetic datasets.\\nPS prompting also outperforms PoT on three arith-\\nmetic datasets. The results suggest that adding\\nmore detailed instructions to the prompt can ef-\\nfectively elicit higher-quality reasoning steps from\\nLLMs.\\nCompared with the few-shot methods, Manual\\nCoT and Auto-CoT, PS+ prompting yields an aver-\\nage accuracy (76.7%) slightly lower than Manual-\\nCoT (77.6%) but higher than Auto-CoT (75.9%).\\nWhile this is an unfair comparison, this result indi-\\ncates that zero-shot prompting can outperform few-\\nshot CoT prompting, which hopefully will spark\\nfurther development of new ways with a less man-\\nual effort to effectively elicit reasoning in LLMs.\\nCommmonsense Reasoning.\\nTable 3 shows the\\nresults on commonsense reasoning datasets: Com-\\nmonsenseQA and StrategyQA. We only include\\nour better zero-shot PS+ prompting strategy in this\\ncomparison. Zero-shot PoT is excluded as it does\\nnot work on this problem. While PS+ prompt-\\ning underperforms Few-Shot-CoT(Manual) on this\\nTable 2: Accuracy comparison on six math reasoning datasets. The best and second best results are boldfaced and\\nunderlined respectively.\\nSetting\\nMethod (text-davinci-003)\\nMultiArith\\nGSM8K\\nAddSub\\nAQuA\\nSingleEq\\nSVAMP\\nAverage\\nZero-Shot\\nCoT\\n83.8\\n56.4\\n85.3\\n38.9\\n88.1\\n69.9\\n70.4\\nPoT\\n92.2\\n57.0\\n85.1\\n43.9\\n91.7\\n70.8\\n73.5\\nPS (ours)\\n87.2\\n58.2\\n88.1\\n42.5\\n89.2\\n72.0\\n72.9\\nPS+ (ours)\\n91.8\\n59.3\\n92.2\\n46.0\\n94.7\\n75.7\\n76.7\\nFew-Shot\\nManual-CoT\\n93.6\\n58.4\\n91.6\\n48.4\\n93.5\\n80.3\\n77.6\\nAuto-CoT\\n95.5\\n57.1\\n90.8\\n41.7\\n92.1\\n78.1\\n75.9\\nTable 3: Accuracy on commonsense reasoning datasets.\\nMethod\\nCSQA StrategyQA\\nFew-Shot-CoT (Manual)\\n78.3\\n71.2\\nZero-shot-CoT\\n65.2\\n63.8\\nZero-shot-PS+ (ours)\\n71.9\\n65.4\\nTable 4: Accuracy on symbolic reasoning datasets.\\nMethod\\nLast Letter Coin Flip\\nFew-Shot-CoT (Manual)\\n70.6\\n100.0\\nZero-shot-CoT\\n64.8\\n96.8\\nZero-shot-PS+ (ours)\\n75.2\\n99.6\\nproblem, it consistently outperforms Zero-shot-\\nCoT on CommonsenseQA (71.9% vs. 65.2%) and\\nStrategyQA (65.4% vs. 63.8%) datasets.\\nSymbolic Reasoning.\\nTable 4 shows the accu-\\nracy of PS+ prompting against Zero-shot-CoT\\nand Few-shot-CoT on symbolic reasoning datasets:\\nLast Letters and Coin Flip. Zero-shot PoT is again\\nexcluded as it is not designed for the problem. On\\nLast Letters, our Zero-shot PS+ prompting (75.2%)\\noutperforms Manual-CoT (70.6%) and Zero-shot-\\nCoT (65.2%). On Coin Flip, Zero-shot PS+ prompt-\\ning (99.6%) is slightly worse than Manual-CoT\\n(100.0%) but outperforms Zero-shot-CoT by a\\ngood margin (96.8%). More examples from the\\nexperiment results can be found in Appendix A.2.\\n4.2\\nAnalysis\\nResults of Prompting with Self-Consistency.\\nSelf-consistency (Wang et al., 2022b) (SC) is pro-\\nposed to reduce randomness in LLM’s output by\\ngenerating N reasoning results and determining\\nthe final answer by majority voting. With SC, the\\nmethods’ results are usually expected to be con-\\nsistent and better. Hence, we evaluate Zero-shot\\nPS+ prompting with SC on GSM8K and SVAMP\\ndatasets. We set the temperature to 0.7 and N to\\n10 for experiments with SC. Figure 4 shows that\\nPS+ prompting with SC (73.7% and 84.4%) sub-\\nstantially outperforms that without SC (58.7% and\\nZero-shot-Cot\\nZero-shot-PS+\\nw/o SC\\nw/o SC\\nw/ SC\\nw/ SC\\nFigure 4: Results of methods with and without self-\\nconsistency (SC) on GSM8K and SVAMP.\\n75.7%) on GSM8K and SVAMP, respectively. The\\nformer also consistently outperforms Zero-shot-\\nCoT with SC (70.7% and 81.7%) on GSM8K and\\nSVAMP, respectively, although Zero-shot CoT also\\nenjoys improvement with the self consistency ap-\\nproach.\\nEffect of Prompts.\\nTable 5 demonstrates a com-\\nparison of the performance of 6 different input\\nprompts. Prompts 1 and 2 are used in Zero-shot\\nCoT and Zero-shot PoT respectively. The rest are\\nvariations of prompts used in Step 1 of the Zero-\\nshot PS+ prompting strategies with greedy decod-\\ning. We observe that Prompt 3 with variables and\\nnumeral extraction performs worse than Prompt\\n1 of Zero-shot-CoT. The reason is that Prompt 3\\ndoesn’t include instructions for devising and com-\\npleting a plan. However, the other prompts of\\nZero-shot-PS+ perform well as we add more in-\\nstructions about intermediate results calculation,\\nplan design, and implementation. The above re-\\nsults conclude that LLMs are capable of generating\\nhigh-quality reasoning text when the prompts in-\\nclude more detailed instructions to guide the LLMs.\\nMore prompts for different reasoning problems can\\nbe found in Appendix A.1.\\nError Analysis.\\nTo qualitatively evaluate the im-\\npact of the Zero-shot-PS+ prompting on calculation\\nerrors and reasoning steps missing errors, we ex-\\namine the distribution of errors on the GSM8K\\ndataset. We first randomly sample 100 problems\\nTable 5: Performance comparison of trigger sentences measured on GSM8K and SVAMP datasets with\\ntext-davinci-003 except for No. 2 (code-davinci-002). (*1) means the trigger sentence used in\\nZero-shot-CoT (Kojima et al., 2022). (*2) means the trigger sentence used in Zero-shot-PoT (Chen et al., 2022).\\nNo.\\nTrigger Sentence\\nGSM8K\\nSVAMP\\n1\\nLet’s think step by step.\\n(*1)\\n56.4\\n69.9\\n2\\nimport math\\nimport numpy as np\\n# Question: example[’question’]\\n# Answer this question by implementing a solver() function.\\ndef solver():\\n# Let’s write a Python program step by step, and then return the answer\\n# Firstly, we need define the following variable:\\n(*2)\\n57.0\\n70.8\\n3\\nExtract variables and assign their corresponding numerals to these variables\\nfirst and then solve the problem step by step.\\n50.5\\n69.5\\n4\\nFirstly, extract variables and their corresponding numerals. Then, calculate\\nintermediate variables. Finally, solve the problem step by step.\\n54.8\\n70.8\\n5\\nLet’s first understand the problem and devise a plan to solve the problem.\\nThen, let’s carry out the plan and solve the problem step by step.\\n58.2\\n72.0\\n6\\nLet’s first understand the problem, extract relevant variables and their\\ncorresponding numerals, and make a plan. Then, let’s carry out the plan,\\ncalculate intermediate variables (pay attention to correct numerical\\ncalculation and commonsense), solve the problem step by step, and show\\nthe answer.\\n59.3\\n75.7\\nTable 6: Distribution of error types of 100 examples\\nfrom GSM8K where Zero-shot-CoT, zero-shot PS (Zero-\\nshot-PS) prompting, and zero-shot PS+ prompting get\\nincorrect final answers.\\nMethod\\nCalculation Missing Semantic\\nZero-shot-CoT\\n7%\\n12%\\n27%\\nZero-shot-PS\\n7%\\n10%\\n26%\\nZero-shot-PS+\\n5%\\n7%\\n27%\\nfrom GSM8K, generate the reasoning text, and\\nextract answers using Zero-Shot-CoT, Zero-shot-\\nPS, and Zero-shot-PS+ prompting strategies. Zero-\\nShot-CoT generated incorrect final answers for 46\\nof the problems, 43 for Zero-shot-PS, and 39 for\\nZero-shot-PS+. Subsequently, we analyze and de-\\ntermine the error types of all these problems as\\nshown in Table 6.\\nThe analysis results show that PS+ prompting\\nachieves the least calculation (5%) and missing-\\nstep (7%) errors, and semantic understanding er-\\nrors comparable to Zero-shot-CoT. Zero-shot-PS\\nhas slightly more errors but is still better than Zero-\\nshot-CoT. Their plan-and-solve prompts thus ef-\\nfectively guide the LLMs to generate clear and\\ncomplete reasoning steps. Moreover, the additional\\ndetailed instructions in PS+ prompting (i.e., “ex-\\ntract relevant variables and their corresponding\\nnumerals” and “calculate intermediate variables”)\\nenable the LLMs to generate high-quality reason-\\ning steps leading to fewer calculation errors.\\nCorrelation Analysis of Generated Reasoning\\nand Error Types.\\nTo obtain deeper insight into\\nthe impact of PS+ prompting on error types, we\\nexamine the correlation between the sub-parts of\\nthe generated reasoning and error types. Specifi-\\ncally, we analyze the existence of variable defini-\\ntion, reasoning plan, and solution in the generated\\nreasoning text and correlate them with the three\\nerror types. The set of problems used for this anal-\\nysis study is the same as that used in the earlier\\nerror type analysis. Figure 5 shows the correla-\\ntion matrix among the existence of variable defi-\\nnitions, plans, solutions and three different types\\nof errors. It is observed that both variable defini-\\ntion and plan existences have a negative correlation\\nwith calculation errors and missing-reasoning-step\\nerrors. The Zero-shot-PS+ prompt can further im-\\nprove the performance of LLMs on mathematical\\nreasoning problems by reducing calculation errors\\nand missing-reasoning-step errors.\\nExploring the Presence of Plans in PS Predic-\\ntions.\\nTo ascertain the presence of a plan in each\\nprediction made by PS, we conducted a random\\nsampling of 100 data examples and examined their\\ncorresponding predictions. Our analysis reveals\\nthat 90 of the 100 predictions indeed incorporated\\na plan. This observation indicates the emergence\\nCalculationStep Missing Semantic\\nVariables\\nPlan\\nSolution\\n-0.41\\n-0.56\\n0.76\\n-0.02\\n-0.83\\n0.7\\n-0.42\\n0.076\\n0.24\\n0.8\\n0.6\\n0.4\\n0.2\\n0.0\\n0.2\\n0.4\\n0.6\\nFigure 5: Correlation analysis of generated reasoning\\nand error types of randomly sampled 100 data examples\\nfrom GSM8K for Zero-shot-PS+.\\nof strong planning abilities in recent LLMs such as\\nGPT-3.5 and GPT-4.\\n5\\nRelated Work\\n5.1\\nReasoning in NLP\\nIt is well known that complex reasoning prob-\\nlems are challenging for NLP models, and such\\nproblems include mathematical reasoning (Cobbe\\net al., 2021; Patel et al., 2021; Ling et al., 2017;\\nKoncel-Kedziorski et al., 2016) (requiring the abil-\\nity to understand mathematical concepts, calcu-\\nlation, and multi-step reasoning), commonsense\\nreasoning (Talmor et al., 2019; Geva et al., 2021)\\n(requiring the ability to make judgments based\\non commonsense knowledge), and logical reason-\\ning (Wei et al., 2022b) (requiring the ability to\\nmanipulate symbols by applying formal logical\\nrules). Before the advent of Large Language mod-\\nels (LLMs), Talmor et al. (2019) trained the NLP\\nmodel using explanations generated by the fine-\\ntuned GPT model and found that the trained model\\nyields better performance on commonsense QA\\nproblems.\\nHendrycks et al. (2021) attempted\\nto fine-tune pretrained language models with la-\\nbeled rationale, but found out that these fine-tuned\\nmodels could not easily generate high-quality rea-\\nsoning steps. Recent work by Wei et al. (2022a)\\nshowed that LLMs demonstrates strong reasoning\\nability when scaled up to tens of billions of pa-\\nrameters, such as GPT-3 (Brown et al., 2020) and\\nPaLM (Chowdhery et al., 2022). These LLMs with\\na few demonstration exemplars can yield impres-\\nsive performance across different NLP tasks. How-\\never, these models still perform poorly in problems\\nthat require multi-step reasoning. This may be due\\nto the fact that the few exemplars provided are in-\\nsufficient to unlock the LLMs’ capabilities.\\n5.2\\nPrompting Methods\\nTo exploit the reasoning ability in LLMs, Wei\\net al. (2022b) propose Chain-of-Thought prompt-\\ning, appending multiple reasoning steps before the\\nanswer to the input question. With this simple\\nfew-shot prompting strategy, LLMs are able to per-\\nform much better in complex reasoning problems.\\nSubsequently, many works (Wang et al., 2022a;\\nSuzgun et al., 2022; Shaikh et al., 2022; Saparov\\nand He, 2022) propose to further improve CoT\\nprompting in different aspects, including prompt\\nformat (Chen et al., 2022), prompt selection (Lu\\net al., 2022), prompt ensemble (Wang et al., 2022b;\\nLi et al., 2022; Weng et al., 2022; Fu et al., 2022),\\nproblem decomposition (Zhou et al., 2022; Khot\\net al., 2022; Dua et al., 2022; Press et al., 2022),\\nand planning (Yao et al., 2022; Huang et al., 2022;\\nWang et al., 2023; Liu et al., 2023; Sun et al., 2023;\\nYao et al., 2023). Chen et al. (2022) introduced\\nPoT prompting to use LLMs with code pre-training\\nto write a program as a rationale for disentangling\\ncomputation from reasoning. To do away with man-\\nual effort, Kojima et al. (2022) proposed Zero-shot-\\nCoT to elicit reasoning step generation without\\nexemplars. To leverage the benefit of demonstra-\\ntion examples and minimize manual effort, Zhang\\net al. (2022) designed Auto-CoT. It first automat-\\nically obtains k examples by clustering the given\\ndataset. It then follows Zero-shot-CoT to gener-\\nate rationales for the selected examples. Finally,\\ndemonstration examples are constructed by adding\\nthe generated rationales to selected examples as\\nCoT prompts. Our work is different from the above\\nworks by focusing on eliciting multi-step reasoning\\nby LLMs in a zero-shot approach. We ask LLMs\\nto write a plan to decompose a complex reasoning\\ntask into multiple reasoning steps. Furthermore,\\nwe introduce detailed instructions to the prompt to\\navoid obvious errors in the reasoning steps. We re-\\nfer readers to the survey (Huang and Chang, 2022)\\nfor more related works.\\n6\\nConclusion\\nIn this paper, we find that Zero-shot-CoT still suf-\\nfers from three pitfalls: calculation errors, missing-\\nreasoning-step errors, and semantic understand-\\ning errors. To address these issues, we introduce\\nplan-and-solve prompting strategies (PS and PS+\\nprompting). They are new zero-shot prompting\\nmethods that guide LLMs to devise a plan that di-\\nvides the entire task into smaller subtasks and then\\ncarries out the subtasks according to the plan. Eval-\\nuation on ten datasets across three types of reason-\\ning problems shows PS+ prompting outperforms\\nthe previous zero-shot baselines and performs on\\npar with few-shot CoT prompting on multiple arith-\\nmetic reasoning datasets. Overall, our results sug-\\ngest that (a) Zero-shot PS+ prompting can generate\\na high-quality reasoning process than Zero-shot-\\nCoT prompting since the PS prompts can provide\\nmore detailed instructions guiding the LLMs to per-\\nform correct reasoning; (b) Zero-shot PS+ prompt-\\ning has the potential to outperform manual Few-\\nshot CoT prompting, which hopefully will spark\\nfurther development of new CoT prompting ap-\\nproaches to elicit reasoning in LLMs. Moreover,\\nPS(+) prompting is a general idea that can be used\\nfor non-reasoning tasks, and refining the plan is\\nalso an interesting idea. We leave them for future\\nwork.\\n7\\nLimitations\\nThere are two limitations to this work. First, it takes\\neffort to design the prompt to guide the LLMs to\\ngenerate correct reasoning steps. The GPT-3 mod-\\nels are sensitive to the expressions in prompts. Thus\\nwe need to carefully design the prompts. Second,\\nthe proposed plan-and-solve prompting can help ad-\\ndress the calculation errors and missing-reasoning-\\nstep errors, but the semantic misunderstanding er-\\nrors still remain. We will explore how to address\\nsemantic misunderstanding errors by prompting\\ninstead of upgrading LLMs in the future.\\n8\\nEthics\\nWe experiment on six math reasoning datasets, in-\\ncluding AQuA (Ling et al., 2017), GSM8K (Cobbe\\net al., 2021), MultiArith, AddSub, SingleEq, and\\nSVAMP (Patel et al., 2021), two commonsense\\nreasoning tasks (CommonsenseQA (Talmor et al.,\\n2019) and StrategyQA (Geva et al., 2021)), and\\ntwo symbolic tasks (Last Letter and Coin Flip (Wei\\net al., 2022b)), where GSM8K and SVAMP use\\nthe MIT License code, AQUA and StrategyQA use\\nthe Apache-2.0 code, the remaining datasets are\\nunspecified.\\nThe proposed prompts do not collect and use\\npersonal information about other individuals. The\\nprompts we used are listed in Appendix.\\nThe\\nprompts in this work do not contain any words\\nthat discriminate against any individual or group.\\nIn this work, prompts would not negatively impact\\nother people’s safety.\\nReferences\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\\nAskell, et al. 2020. Language models are few-shot\\nlearners. Advances in neural information processing\\nsystems, 33:1877–1901.\\nWenhu Chen, Xueguang Ma, Xinyi Wang, and\\nWilliam W Cohen. 2022.\\nProgram of thoughts\\nprompting: Disentangling computation from reason-\\ning for numerical reasoning tasks. arXiv preprint\\narXiv:2211.12588.\\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\\nPaul Barham, Hyung Won Chung, Charles Sutton,\\nSebastian Gehrmann, et al. 2022. PaLM: Scaling\\nlanguage modeling with pathways. arXiv preprint\\narXiv:2204.02311.\\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavar-\\nian, Jacob Hilton, Reiichiro Nakano, Christopher\\nHesse, and John Schulman. 2021.\\nTraining veri-\\nfiers to solve math word problems. arXiv preprint\\narXiv:2110.14168.\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\\nKristina Toutanova. 2019. BERT: Pre-training of\\ndeep bidirectional transformers for language under-\\nstanding. In Proceedings of NAACL, pages 4171–\\n4186.\\nDheeru Dua, Shivanshu Gupta, Sameer Singh, and\\nMatt Gardner. 2022.\\nSuccessive prompting for\\ndecomposing complex questions.\\narXiv preprint\\narXiv:2212.04092.\\nYao Fu, Hao Peng, Ashish Sabharwal, Peter Clark,\\nand Tushar Khot. 2022. Complexity-based prompt-\\ning for multi-step reasoning.\\narXiv preprint\\narXiv:2210.00720.\\nMor Geva, Daniel Khashabi, Elad Segal, Tushar Khot,\\nDan Roth, and Jonathan Berant. 2021. Did aristotle\\nuse a laptop? a question answering benchmark with\\nimplicit reasoning strategies. TACL, 9:346–361.\\nJunxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-\\nKirkpatrick, and Graham Neubig. 2021. Towards a\\nunified view of parameter-efficient transfer learning.\\narXiv preprint arXiv:2110.04366.\\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul\\nArora, Steven Basart, Eric Tang, Dawn Song, and Ja-\\ncob Steinhardt. 2021. Measuring mathematical prob-\\nlem solving with the math dataset. arXiv preprint\\narXiv:2103.03874.\\nMohammad Javad Hosseini, Hannaneh Hajishirzi, Oren\\nEtzioni, and Nate Kushman. 2014. Learning to solve\\narithmetic word problems with verb categorization.\\nIn EMNLP, pages 523–533.\\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\\nBruna Morrone, Quentin De Laroussilhe, Andrea\\nGesmundo, Mona Attariyan, and Sylvain Gelly. 2019.\\nParameter-efficient transfer learning for nlp. In In-\\nternational Conference on Machine Learning, pages\\n2790–2799. PMLR.\\nJie Huang and Kevin Chen-Chuan Chang. 2022. To-\\nwards reasoning in large language models: A survey.\\narXiv preprint arXiv:2212.10403.\\nWenlong Huang, Pieter Abbeel, Deepak Pathak, and\\nIgor Mordatch. 2022. Language models as zero-shot\\nplanners: Extracting actionable knowledge for em-\\nbodied agents. In International Conference on Ma-\\nchine Learning, pages 9118–9147. PMLR.\\nTushar Khot, Harsh Trivedi, Matthew Finlayson, Yao\\nFu, Kyle Richardson, Peter Clark, and Ashish Sab-\\nharwal. 2022. Decomposed prompting: A modular\\napproach for solving complex tasks. arXiv preprint\\narXiv:2210.02406.\\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\\ntaka Matsuo, and Yusuke Iwasawa. 2022. Large lan-\\nguage models are zero-shot reasoners. arXiv preprint\\narXiv:2205.11916.\\nRik Koncel-Kedziorski, Hannaneh Hajishirzi, Ashish\\nSabharwal, Oren Etzioni, and Siena Dumas Ang.\\n2015. Parsing algebraic word problems into equa-\\ntions. Transactions of the Association for Computa-\\ntional Linguistics, 3:585–597.\\nRik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate\\nKushman, and Hannaneh Hajishirzi. 2016. MAWPS:\\nA math word problem repository. In Proceedings of\\nNAACL, pages 1152–1157.\\nYifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen,\\nJian-Guang Lou, and Weizhu Chen. 2022. On the\\nadvance of making language models better reasoners.\\narXiv preprint arXiv:2206.02336.\\nWang Ling, Dani Yogatama, Chris Dyer, and Phil Blun-\\nsom. 2017. Program induction by rationale genera-\\ntion: Learning to solve and explain algebraic word\\nproblems. In Proceedings of the 55th Annual Meet-\\ning of the Association for Computational Linguistics\\n(Volume 1: Long Papers), pages 158–167.\\nBo Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu,\\nShiqi Zhang, Joydeep Biswas, and Peter Stone.\\n2023. Llm+ p: Empowering large language mod-\\nels with optimal planning proficiency. arXiv preprint\\narXiv:2304.11477.\\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\\nRoberta: A robustly optimized bert pretraining ap-\\nproach. arXiv preprint arXiv:1907.11692.\\nPan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu,\\nSong-Chun Zhu, Tanmay Rajpurohit, Peter Clark,\\nand Ashwin Kalyan. 2022. Dynamic prompt learning\\nvia policy gradient for semi-structured mathematical\\nreasoning. arXiv preprint arXiv:2209.14610.\\nArkil Patel, Satwik Bhattamishra, and Navin Goyal.\\n2021. Are NLP models really able to solve simple\\nmath word problems?\\nIn Proceedings of NAACL,\\npages 2080–2094.\\nOfir Press, Muru Zhang, Sewon Min, Ludwig Schmidt,\\nNoah A Smith, and Mike Lewis. 2022. Measuring\\nand narrowing the compositionality gap in language\\nmodels. arXiv preprint arXiv:2210.03350.\\nSubhro Roy and Dan Roth. 2016.\\nSolving gen-\\neral arithmetic word problems.\\narXiv preprint\\narXiv:1608.01413.\\nAbulhair Saparov and He He. 2022. Language models\\nare greedy reasoners: A systematic formal analysis of\\nchain-of-thought. arXiv preprint arXiv:2210.01240.\\nOmar Shaikh, Hongxin Zhang, William Held, Michael\\nBernstein, and Diyi Yang. 2022. On second thought,\\nlet’s not think step by step! bias and toxicity in zero-\\nshot reasoning. arXiv preprint arXiv:2212.08061.\\nSimeng Sun, Yang Liu, Shuohang Wang, Chenguang\\nZhu, and Mohit Iyyer. 2023. Pearl: Prompting large\\nlanguage models to plan and execute actions over\\nlong documents.\\nTianxiang Sun, Yunfan Shao, Hong Qian, Xuanjing\\nHuang, and Xipeng Qiu. 2022. Black-box tuning\\nfor language-model-as-a-service.\\narXiv preprint\\narXiv:2201.03514.\\nMirac Suzgun, Nathan Scales, Nathanael Schärli, Se-\\nbastian Gehrmann, Yi Tay, Hyung Won Chung,\\nAakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny\\nZhou, et al. 2022. Challenging big-bench tasks and\\nwhether chain-of-thought can solve them.\\narXiv\\npreprint arXiv:2210.09261.\\nAlon Talmor, Jonathan Herzig, Nicholas Lourie, and\\nJonathan Berant. 2019. Commonsenseqa: A question\\nanswering challenge targeting commonsense knowl-\\nedge. In Proceedings of NAACL-HLT, pages 4149–\\n4158.\\nRomal Thoppilan, Daniel De Freitas, Jamie Hall, Noam\\nShazeer, Apoorv Kulshreshtha, Heng-Tze Cheng,\\nAlicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al.\\n2022. Lamda: Language models for dialog applica-\\ntions. arXiv preprint arXiv:2201.08239.\\nBoshi Wang, Sewon Min, Xiang Deng, Jiaming Shen,\\nYou Wu, Luke Zettlemoyer, and Huan Sun. 2022a.\\nTowards understanding chain-of-thought prompting:\\nAn empirical study of what matters. arXiv preprint\\narXiv:2212.10001.\\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le,\\nEd Chi, and Denny Zhou. 2022b. Self-consistency\\nimproves chain of thought reasoning in language\\nmodels. arXiv preprint arXiv:2203.11171.\\nZihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, and\\nYitao Liang. 2023. Describe, explain, plan and se-\\nlect: Interactive planning with large language models\\nenables open-world multi-task agents. arXiv preprint\\narXiv:2302.01560.\\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,\\nBarret Zoph, Sebastian Borgeaud, Dani Yogatama,\\nMaarten Bosma, Denny Zhou, Donald Metzler, et al.\\n2022a. Emergent abilities of large language models.\\narXiv preprint arXiv:2206.07682.\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\\nBosma, Ed Chi, Quoc Le, and Denny Zhou. 2022b.\\nChain of thought prompting elicits reasoning in large\\nlanguage models. In Thirty-sixth Conference on Neu-\\nral Information Processing Systems (NeurIPS 2022).\\nYixuan Weng, Minjun Zhu, Shizhu He, Kang Liu,\\nand Jun Zhao. 2022. Large language models are\\nreasoners with self-verification.\\narXiv preprint\\narXiv:2212.09561.\\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,\\nThomas L. Griffiths,\\nYuan Cao,\\nand Karthik\\nNarasimhan. 2023.\\nTree of thoughts: Deliberate\\nproblem solving with large language models.\\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak\\nShafran, Karthik Narasimhan, and Yuan Cao. 2022.\\nReact: Synergizing reasoning and acting in language\\nmodels. ArXiv, abs/2210.03629.\\nZhuosheng Zhang, Aston Zhang, Mu Li, and Alex\\nSmola. 2022. Automatic chain of thought prompt-\\ning in large language models.\\narXiv preprint\\narXiv:2210.03493.\\nDenny Zhou, Nathanael Schärli, Le Hou, Jason Wei,\\nNathan Scales, Xuezhi Wang, Dale Schuurmans,\\nOlivier Bousquet, Quoc Le, and Ed Chi. 2022.\\nLeast-to-most prompting enables complex reason-\\ning in large language models.\\narXiv preprint\\narXiv:2205.10625.\\nA\\nAppendix\\nThis section includes two parts: (1) Results of all\\nprompts we have tried; (2) Example texts generated\\nby Zero-shot-PS+. Unless otherwise mentioned,\\nwe use GPT3 (text-davinci-003) model.\\nA.1\\nResults of All Trigger Sentences\\nTables 7 to 16 list the results of all prompts we have\\ntried for each dataset.\\nA.2\\nExample Outputs by Zero-shot-PS+\\nTables 17 to 25 list example outputs generated by\\nZero-shot-PS+ for each dataset.\\nTable 7:\\nPerformance comparison of prompts used in Step 1 of Zero-shot-PS+ prompting with\\ntext-davinci-003 on AQuA.\\nNo.\\nTrigger Setence\\nAccuracy\\n1\\nLet’s first understand the problem and devise a plan to solve the problem. Then, let’s carry out the plan\\nto solve the problem step by step.\\n42.5\\n2\\nLet’s first understand the problem, extract all relevant variables and their corresponding numerals\\ncarefully, and devise a plan. Then, let’s carry out the plan, calculate intermediate variables (pay attention\\nto correct numerical calculation and common sense), solve the problem step by step carefully, and show\\nthe answer.\\n42.9\\n3\\nLet’s first understand the problem, extract relevant correct variables and their correct corresponding\\nnumerals, and devise complete plans. Then, let’s carry out the plan, calculate intermediate variables\\nincluding extracted variables (pay attention to correct numerical calculation and common sense), solve\\nthe problem by single equations, and show the answer.\\n43.7\\n4\\nLet’s first understand the problem, extract relevant variables and their corresponding numerals, and\\nmake a complete plan.Then, let’s carry out the plan, calculate intermediate variables (pay attention to\\ncorrect numerical calculation and commonsense), solve the problem step by step, and show the answer.\\n46.0\\nTable 8:\\nPerformance comparison of prompts used in Step 1 of Zero-shot-PS+ prompting with\\ntext-davinci-003 on GSM8K.\\nNo.\\nTrigger Setence\\nAccuracy\\n1\\nLet’s first understand the problem and devise a plan to solve the problem. Then, let’s carry out the plan\\nto solve the problem step by step.\\n58.2\\n2\\nLet’s first understand the problem, extract relevant variables and their corresponding numerals, and\\ndevise a plan. Then, let’s carry out the plan, calculate intermediate variables (pay attention to correct\\nnumeral calculation and commonsense), solve the problem step by step, and show the answer.\\n58.7\\n3\\nLet’s first understand the problem, extract relevant variables and their corresponding numerals, and\\nmake a complete plan.Then, let’s carry out the plan, calculate intermediate variables (pay attention to\\ncorrect numerical calculation and commonsense), solve the problem step by step, and show the answer.\\n59.3\\nTable 9:\\nPerformance comparison of prompts used in Step 1 of Zero-shot-PS+ prompting with\\ntext-davinci-003 on MultiArith.\\nNo.\\nTrigger Setence\\nAccuracy\\n1\\nLet’s first understand the problem and devise a plan to solve the problem. Then, let’s carry out the plan\\nto solve the problem step by step.\\n87.2\\n2\\nLet’s first understand the problem, extract relevant variables and their corresponding numerals, and\\ndevise a plan. Then, let’s carry out the plan, calculate intermediate variables (pay attention to correct\\nnumeral calculation and commonsense), solve the problem step by step, and show the answer.\\n88.3\\n3\\nLet’s first understand the problem, extract relevant variables and their corresponding numerals, and\\ndevise a complete plan. Then, let’s carry out the plan, calculate intermediate variables (pay attention\\nto the correctness of the calculation and common sense), solve the problem step by step, and show\\nthe answer.\\n90.5\\n4\\nLet’s first understand the problem, extract relevant variables and their corresponding numerals, and\\ndevise a complete plan. Then, let’s carry out the plan, calculate intermediate variables (pay attention to\\ncorrect numerical calculation and commonsense), solve the problem step by step, and show the answer.\\n91.8\\nTable 10:\\nPerformance comparison of prompts used in Step 1 of Zero-shot-PS+ prompting with\\ntext-davinci-003 on SVAMP.\\nNo.\\nTrigger Setence\\nAccuracy\\n1\\nLet’s first understand the problem and devise a plan to solve the problem. Then, let’s carry out the plan\\nto solve the problem step by step.\\n72.0\\n2\\nLet’s first understand the problem, extract relevant variables and their corresponding numerals, and\\ndevise a plan. Then, let’s carry out the plan, calculate intermediate variables (pay attention to correct\\nnumeral calculation and commonsense), solve the problem step by step, and show the answer.\\n75.4\\n3\\nLet’s first understand the problem, extract relevant variables and their corresponding numerals, and\\nmake a complete plan.Then, let’s carry out the plan, calculate intermediate variables (pay attention to\\ncorrect numerical calculation and commonsense), solve the problem step by step, and show the answer.\\n75.7\\nTable 11:\\nPerformance comparison of prompts used in Step 1 of Zero-shot-PS+ prompting with\\ntext-davinci-003 on AddSub.\\nNo.\\nTrigger Setence\\nAccuracy\\n1\\nLet’s first understand the problem and devise a plan to solve the problem. Then, let’s carry out the plan\\nto solve the problem step by step.\\n87.3\\n2\\nLet’s first understand the problem, extract relevant variables and their corresponding numerals, and\\ndevise a complete plan.Then, let’s carry out the plan, calculate intermediate variables (pay attention to\\ncorrect numerical calculation and commonsense), solve the problem step by step, and show the answer.\\n87.8\\n3\\nLet’s first understand the problem, extract relevant variables and their corresponding numerals, and\\ndevise a plan. Then, let’s carry out the plan, calculate intermediate variables (pay attention to correct\\nnumeral calculation and commonsense), solve the problem step by step, and show the answer.\\n92.2\\nTable 12:\\nPerformance comparison of prompts used in Step 1 of Zero-shot-PS+ prompting with\\ntext-davinci-003 on SingleEq.\\nNo.\\nTrigger Setence\\nAccuracy\\n1\\nLet’s first understand the problem and devise a plan to solve the problem. Then, let’s carry out the plan\\nto solve the problem step by step.\\n92.3\\n2\\nLet’s first understand the problem, extract relevant variables and their corresponding numerals, and\\ndevise a plan. Then, let’s carry out the plan, calculate intermediate variables (pay attention to correct\\nnumeral calculation and commonsense), solve the problem step by step, and show the answer.\\n94.7\\nTable 13:\\nPerformance comparison of prompts used in Step 1 of Zero-shot-PS+ prompting with\\ntext-davinci-003 on CSQA.\\nNo.\\nTrigger Setence\\nAccuracy\\n1\\nLet’s devise a plan and solve the problem step by step.\\n67.4\\n2\\nLet’s first understand the problem, extract relevant variables and their corresponding numerals, and\\ndevise a complete plan.Then, let’s carry out the plan, calculate intermediate variables (pay attention to\\ncorrect numerical calculation and commonsense), solve the problem step by step, and show the answer.\\n71.9\\nTable 14:\\nPerformance comparison of prompts used in Step 1 of Zero-shot-PS+ prompting with\\ntext-davinci-003 on StrategyQA.\\nNo.\\nTrigger Setence\\nAccuracy\\n1\\nLet’s devise a plan and solve the problem step by step.\\n61.5\\n2\\nLet’s devise a complete plan. Then, let’s carry out the plan, solve the problem step by step, and\\nshow the answer.\\n63.0\\n3\\nLet’s first prepare relevant information and make a plan. Then, let’s answer the question step by step\\n(pay attention to commonsense and logical coherence).\\n65.4\\nTable 15:\\nPerformance comparison of prompts used in Step 1 of Zero-shot-PS+ prompting with\\ntext-davinci-003 on Last Letters.\\nNo.\\nTrigger Setence\\nAccuracy\\n1\\nLet’s devise a plan and solve the problem step by step.\\n75.2\\nTable 16:\\nPerformance comparison of prompts used in Step 1 of Zero-shot-PS+ prompting with\\ntext-davinci-003 on Coin Flip.\\nNo.\\nTrigger Setence\\nAccuracy\\n1\\nLet’s devise a complete plan. Then, let’s carry out the plan, solve the problem step by step,\\nand show the answer.\\n70.6\\n2\\nLet’s first devise a plan, then solve the problem step by step.\\n72.6\\n3\\nLet’s first devise a plan, then solve the problem step by step.(Distinguish between tail up and\\nhead up)\\n84.0\\n4\\nLet’s first understand the problem and devise a complete plan.Then, let’s carry out the plan,\\nreason problem step by step. Every step answer the subquestion \"does the person flip and\\nwhat is the coin current state?\", and according to the last coin state, give the final answer\\n(pay attention that the coin turns state at every flip).\\nPlan:\\nStep 1:\\n94.2\\n5\\nLet’s first understand the problem and devise a complete plan. Then, let’s carry out the plan,\\nreason problem step by step. Every step answer the subquestion \"does the person flip and what\\nis the coin current state?\", and according to the coin last state give the final answer(pay attention\\nto every flip, the coin turns state).\\nPlan:\\nStep 1:\\n99.0\\n6\\nLet’s first understand the problem and devise a complete plan. Then, let’s carry out the plan and\\nreason problem step by step. Every step answer the subquestion, \"does the person flip and what is\\nthe coin’s current state?\". According to the coin’s last state, give the final answer (pay attention to\\nevery flip and the coin’s turning state).\\n99.6\\nTable 17: Example outputs by Zero-shot-PS+ for AQuA.\\nQ: The average wages of a worker during a fortnight comprising of 15 consecutive working days was\\n$90 per day. During the first 7 days, his average wage was $87 per day and the average wage during\\nthe last 7 days was $92 per day. What was his wage on the 8th day? Answer Choices: (A) $83 (B)\\n$92 (C) $90 (D) $97 (E) $104\\nA: Let’s first understand the problem, extract relevant variables and their corresponding numerals, and\\nmake a complete plan.Then, let’s carry out the plan, calculate intermediate variables (pay attention\\nto correct numerical calculation and commonsense), solve the problem step by step, and show the\\nanswer.\\nOutput:\\nVariables:\\nAverage wage during the first 7 days = 87\\nAverage wage during the last 7 days = 92\\nAverage wage during the fortnight = 90\\nPlan:\\nCalculate the total wages earned during\\nthe first 7 days.\\nCalculate the total wages earned during\\nthe last 7 days.\\nCalculate the total wages earned during\\nthe fortnight.\\nCalculate the wage on the 8th day.\\nCalculation:\\nTotal wages earned during the first 7 days = 7 x 87 = 609\\nTotal wages earned during the last 7 days = 7 x 92 = 644\\nTotal wages earned during the fortnight = 15 x 90 = 1350\\nWage on the 8th day = (1350 - 609 - 644) / 1 = 97\\nAnswer: (D) $97.\\nQ: If 27 bottles of soda cost A cents, how much will B bottles cost in dollars? Answer Choices: (A)\\nAB/2700 (B) 27/AB (C) AB/270 (D) 2700/AB (E) 100AB/27\\nA: Let’s first understand the problem, extract relevant variables and their corresponding numerals, and\\nmake a complete plan.Then, let’s carry out the plan, calculate intermediate variables (pay attention\\nto correct numerical calculation and commonsense), solve the problem step by step, and show the\\nanswer.\\nOutput:\\nGiven:\\n27 bottles of soda cost A cents\\nB bottles of soda\\nPlan:\\n1. Convert A cents to dollars\\n2. Calculate the cost of B bottles of soda\\nSolution:\\n1. Convert A cents to dollars: A cents = A/100 dollars\\n2. Calculate the cost of B bottles of soda: B bottles of soda = (A/100) * (B/27) = AB/2700 dollars\\nAnswer: (A) AB/2700\\nTable 18: Example outputs by Zero-shot-PS+ for GSM8K.\\nQ: James decides to run 3 sprints 3 times a week. He runs 60 meters each sprint. How many total\\nmeters does he run a week?\\nA: Let’s first understand the problem, extract relevant variables and their corresponding numerals, and\\nmake a complete plan.Then, let’s carry out the plan, calculate intermediate variables (pay attention\\nto correct numerical calculation and commonsense), solve the problem step by step, and show the\\nanswer.\\nOutput:\\nGiven:\\nJames runs 3 sprints 3 times a week.\\nEach sprint is 60 meters.\\nPlan:\\nWe need to calculate the total meters run by James in a week.\\nCalculation:\\nTotal number of sprints run by James in a week = 3 sprints x 3 times = 9 sprints\\nTotal meters run by James in a week = 9 sprints x 60 meters = 540 meters\\nAnswer:\\nJames runs 540 meters in a week.\\nQ: In a dance class of 20 students, 20% enrolled in contemporary dance, 25% of the remaining\\nenrolled in jazz dance, and the rest enrolled in hip-hop dance. What percentage of the entire students\\nenrolled in hip-hop dance?\\nA: Let’s first understand the problem, extract relevant variables and their corresponding numerals, and\\nmake a complete plan.Then, let’s carry out the plan, calculate intermediate variables (pay attention\\nto correct numerical calculation and commonsense), solve the problem step by step, and show the\\nanswer.\\nOutput:\\nGiven:\\nTotal number of students = 20\\nPercentage of students enrolled in contemporary dance = 20%\\nPercentage of students enrolled in jazz dance = 25%\\nPlan:\\n1. Calculate the number of students enrolled in contemporary dance.\\n2. Calculate the number of students remaining after subtracting the number of students enrolled in\\ncontemporary dance.\\n3. Calculate the number of students enrolled in jazz dance.\\n4. Calculate the number of students enrolled in hip-hop dance.\\n5. Calculate the percentage of students enrolled in hip-hop dance.\\nCalculation:\\n1. Number of students enrolled in contemporary dance = 20% of 20 = 20% × 20 = 4\\n2. Number of students remaining after subtracting the number of students enrolled in contemporary\\ndance = 20 - 4 = 16\\n3. Number of students enrolled in jazz dance = 25% of 16 = 25% × 16 = 4\\n4. Number of students enrolled in hip-hop dance = 16 - 4 = 12\\n5. Percentage of students enrolled in hip-hop dance = 12/20 × 100% = 60%\\nAnswer:\\n60% of the entire students enrolled in hip-hop dance.\\nTable 19: Example outputs by Zero-shot-PS+ for MultiArith.\\nQ: The school cafeteria ordered 42 red apples and 7 green apples for students lunches. But, if only 9\\nstudents wanted fruit, how many extra did the cafeteria end up with?\\nA: Let’s first understand the problem, extract relevant variables and their corresponding numerals, and\\ndevise a complete plan.Then, let’s carry out the plan, calculate intermediate variables (pay attention\\nto correct numerical calculation and commonsense), solve the problem step by step, and show the\\nanswer.\\nOutput:\\nVariables:\\nRed apples = 42\\nGreen apples = 7\\nStudents wanting fruit = 9\\nPlan:\\nCalculate the total number of apples ordered by the cafeteria\\nSubtract the total number of apples ordered by the cafeteria from the number of students wanting fruit\\nCalculation:\\nTotal number of apples ordered by the cafeteria = 42 + 7 = 49\\nAnswer:\\nExtra apples = 49 - 9 = 40\\nQ: Carol and her mom were picking carrots from their garden. Carol picked 29 and her mother picked\\n16. If only 38 of the carrots were good, how many bad carrots did they have?\\nA: Let’s first understand the problem, extract relevant variables and their corresponding numerals, and\\ndevise a complete plan.Then, let’s carry out the plan, calculate intermediate variables (pay attention\\nto correct numerical calculation and commonsense), solve the problem step by step, and show the\\nanswer.\\nOutput:\\nVariables:\\nCarol: 29\\nMom: 16\\nGood Carrots: 38\\nPlan:\\nWe can calculate the number of bad carrots by subtracting the number of good carrots from the total\\nnumber of carrots picked.\\nCalculation:\\nTotal Carrots Picked = Carol + Mom = 29 + 16 = 45\\nBad Carrots = Total Carrots Picked - Good Carrots = 45 - 38 = 7\\nAnswer:\\nThere were 7 bad carrots.\\nTable 20: Example outputs by Zero-shot-PS+ for SVAMP.\\nQ: Mom buys 70 white t - shirts in total. If white t - shirts can be purchased in packages and mom\\nbuys 14 packages. How many white t - shirts does each package have?\\nA: Let’s first understand the problem, extract relevant variables and their corresponding numerals,\\nand make and device a complete plan.Then, let’s carry out the plan, calculate intermediate variables\\n(pay attention to correct numerical calculation and commonsense), solve the problem step by step,\\nand show the answer.\\nOutput:\\nVariables:\\nTotal number of white t-shirts = 70\\nNumber of packages = 14\\nPlan:\\nWe can use the equation:\\nTotal number of white t-shirts = Number of packages x Number of t-shirts per package\\nCalculation:\\nNumber of t-shirts per package = 70/14 = 5\\nAnswer:\\nEach package has 5 white t-shirts.\\nQ: Jesse’s rooms are 19 feet long and 18 feet wide. If she has 20 rooms in all. How much carpet does\\nshe need to cover the floors of all rooms?\\nA: Let’s first understand the problem, extract relevant variables and their corresponding numerals,\\nand make and device a complete plan.Then, let’s carry out the plan, calculate intermediate variables\\n(pay attention to correct numerical calculation and commonsense), solve the problem step by step,\\nand show the answer.\\nOutput:\\nRelevant Variables:\\nNumber of rooms: 20\\nLength of each room: 19 feet\\nWidth of each room: 18 feet\\nPlan:\\nStep 1: Calculate the area of each room.\\nStep 2: Multiply the area of each room by the number of rooms.\\nStep 3: Show the answer.\\nCalculation:\\nStep 1: Area of each room = Length x Width = 19 feet x 18 feet = 342 square feet\\nStep 2: Total area of all rooms = Area of each room x Number of rooms = 342 square feet x 20 rooms\\n= 6840 square feet\\nAnswer: Jesse needs 6840 square feet of carpet to cover the floors of all rooms.\\nTable 21: Example outputs by Zero-shot-PS+ for AddSub.\\nQ: There were 28 bales of hay in the barn. Tim stacked bales in the barn today. There are now 54\\nbales of hay in the barn. How many bales did he store in the barn ?\\nA: Let’s first understand the problem, extract relevant variables and their corresponding numerals,\\nand devise a plan. Then, let’s carry out the plan, calculate intermediate variables (pay attention to\\ncorrect numeral calculation and commonsense), solve the problem step by step, and show the answer.\\nOutput:\\nGiven:\\nInitial number of bales of hay in the barn = 28\\nNumber of bales of hay in the barn after Tim stacked them = 54\\nPlan:\\nCalculate the number of bales of hay Tim stored in the barn by subtracting the initial number of bales\\nof hay from the number of bales of hay after Tim stacked them.\\nCalculation:\\nNumber of bales of hay Tim stored in the barn = 54 - 28 = 26\\nAnswer:\\nTim stored 26 bales of hay in the barn.\\nQ: There were 6 roses in the vase. Mary cut some roses from her flower garden. There are now 16\\nroses in the vase. How many roses did she cut ?\\nA: Let’s first understand the problem, extract relevant variables and their corresponding numerals,\\nand devise a plan. Then, let’s carry out the plan, calculate intermediate variables (pay attention to\\ncorrect numeral calculation and commonsense), solve the problem step by step, and show the answer.\\nOutput:\\nGiven:\\nInitial number of roses in the vase = 6\\nFinal number of roses in the vase = 16\\nPlan:\\nSubtract the initial number of roses from the final number of roses to calculate the number of roses\\nMary cut.\\nCalculation:\\nNumber of roses Mary cut = 16 - 6 = 10\\nAnswer:\\nMary cut 10 roses from her flower garden.\\nTable 22: Example outputs by Zero-shot-PS+ for SingleEq.\\nQ: After eating at the restaurant, Sally, Sam, and Alyssa decided to divide the bill evenly. If each\\nperson paid 45 dollars, what was the total of the bill ?\\nA: Let’s first understand the problem, extract relevant variables and their corresponding numerals,\\nand devise a plan. Then, let’s carry out the plan, calculate intermediate variables (pay attention to\\ncorrect numeral calculation and commonsense), solve the problem step by step, and show the answer.\\nOutput:\\nProblem: After eating at the restaurant, Sally, Sam, and Alyssa decided to divide the bill evenly. If\\neach person paid 45 dollars, what was the total of the bill?\\nVariables:\\n- Total of the bill (T)\\n- Number of people (N)\\n- Amount paid by each person (A)\\nNumerals:\\n- N = 3\\n- A = 45\\nPlan:\\n- Calculate the total of the bill (T) by multiplying the number of people (N) by the amount paid by\\neach person (A).\\nCalculation:\\nT = N x A\\nT = 3 x 45\\nT = 135\\nAnswer: The total of the bill was 135 dollars.\\nQ: Mary is baking a cake. The recipe wants 8 cups of flour. She already put in 2 cups. How many\\nmore cups does she need to add ?\\nA: Let’s first understand the problem, extract relevant variables and their corresponding numerals,\\nand devise a plan. Then, let’s carry out the plan, calculate intermediate variables (pay attention to\\ncorrect numeral calculation and commonsense), solve the problem step by step, and show the answer.\\nOutput:\\nMary needs to add 8 cups of flour to the cake. She has already put in 2 cups.\\nPlan:\\nSubtract the amount of flour Mary has already put in from the total amount of flour needed.\\nCalculation:\\n8 cups (total amount of flour needed) - 2 cups (amount of flour Mary has already put in) = 6 cups\\n(amount of flour Mary needs to add)\\nAnswer: Mary needs to add 6 more cups of flour.\\nTable 23: Example outputs by Zero-shot-PS+ for CommonsenseQA.\\nQ: What is a place that has a bench nestled in trees? Answer Choices: (A) state park (B) bus stop (C)\\nbus depot (D) statue (E) train station\\nA: Let’s first understand the problem, extract relevant variables and their corresponding numerals, and\\ndevise a complete plan.Then, let’s carry out the plan, calculate intermediate variables (pay attention\\nto correct numerical calculation and commonsense), solve the problem step by step, and show the\\nanswer.\\nOutput:\\nThe question is asking for a place that has a bench nestled in trees. The answer choices are (A) state\\npark, (B) bus stop, (C) bus depot, (D) statue, and (E) train station.\\nThe answer is (A) state park. State parks are typically located in natural areas with trees, and they\\noften have benches for visitors to sit and enjoy the scenery.\\nQ: Where is a good idea but not required to have a fire extinguisher? Answer Choices: (A) school bus\\n(B) boat (C) house (D) hospital (E) school\\nA: Let’s first understand the problem, extract relevant variables and their corresponding numerals, and\\ndevise a complete plan.Then, let’s carry out the plan, calculate intermediate variables (pay attention\\nto correct numerical calculation and commonsense), solve the problem step by step, and show the\\nanswer.\\nOutput:\\nThe question is asking for a good idea to have a fire extinguisher, but it is not required. The answer\\nchoices are (A) school bus, (B) boat, (C) house, (D) hospital, and (E) school.\\nThe best answer is (C) house. Having a fire extinguisher in a house is a good idea, but it is not\\nrequired. Fire extinguishers are not typically required in school buses, boats, hospitals, or schools.\\nTable 24: Example outputs by Zero-shot-PS+ for StrategyQA.\\nQ: Does the country that received the most gold medals during the 1976 Olympics still exist?\\nA: Let’s first prepare relevant information and make a plan. Then, let’s answer the question step by\\nstep (pay attention to commonsense and logical coherence).\\nOutput:\\nRelevant information:\\n- The 1976 Olympics were held in Montreal, Canada.\\n- The country that received the most gold medals during the 1976 Olympics was the Soviet Union.\\nPlan:\\n1. Determine if the Soviet Union still exists.\\n2. If the Soviet Union still exists, answer the question.\\n3. If the Soviet Union does not exist, determine which countries it has been replaced by. Answer:\\n1. The Soviet Union no longer exists.\\n2. No, the country that received the most gold medals during the 1976 Olympics does not still exist.\\n3. The Soviet Union has been replaced by 15 independent countries, including Russia, Ukraine,\\nBelarus, and Kazakhstan.\\nQ:Is the Mona Lisa in the same museum as the Venus de Milo?\\nA: Let’s first prepare relevant information and make a plan. Then, let’s answer the question step by\\nstep (pay attention to commonsense and logical coherence).\\nOutput:\\nRelevant information:\\nMona Lisa: Located in the Louvre Museum in Paris, France\\nVenus de Milo: Located in the Louvre Museum in Paris, France\\nPlan:\\nStep 1: Identify the locations of the Mona Lisa and the Venus de Milo\\nStep 2: Compare the locations\\nAnswer:\\nYes, the Mona Lisa and the Venus de Milo are both located in the Louvre Museum in Paris, France.\\nTable 25: Example outputs by Zero-shot-PS+ for Last Letters.\\nQ: Take the last letters of each words in \"Lino Mariel Aditya Elisabeth\" and concatenate them\\nA:Let’s devise a plan and solve the problem step by step.\\nOutput:\\nStep 1: Take the last letters of each word in the given string.\\nLino: o\\nMariel: l\\nAditya: a\\nElisabeth: h\\nStep 2: Concatenate the last letters.\\nAnswer: olah\\nQ: Take the last letters of each words in \"Jacky Socorro Mark Wanda\" and concatenate them\\nA:Let’s devise a plan and solve the problem step by step.\\nOutput:\\nStep 1: Take the last letters of each word.\\nJacky: y\\nSocorro: o\\nMark: k\\nWanda: a\\nStep 2: Concatenate the letters.\\nAnswer: yoka\\nTable 26: Example outputs by Zero-shot-PS+ for Coin Flip.\\nQ: A coin is heads up. Walter does not flip the coin. Mikayla does not flip the coin. Larry does not flip\\nthe coin. Ryan does not flip the coin. Is the coin still heads up? Note that \"flip\" here means \"reverse\".\\nA: Let’s first understand the problem and devise a complete plan. Then, let’s carry out the plan and\\nreason problem step by step. Every step answer the subquestion, \"does the person flip and what is the\\ncoin’s current state?\". According to the coin’s last state, give the final answer (pay attention to every\\nflip and the coin’s turning state).\\nOutput:\\nPlan:\\nStep 1: Walter does not flip the coin.\\nStep 2: Mikayla does not flip the coin.\\nStep 3: Larry does not flip the coin.\\nStep 4: Ryan does not flip the coin.\\nAnswer: Yes, the coin is still heads up.\\nExplanation:\\nStep 1: Walter does not flip the coin. Therefore, the coin is still heads up.\\nStep 2: Mikayla does not flip the coin. Therefore, the coin is still heads up.\\nStep 3: Larry does not flip the coin. Therefore, the coin is still heads up.\\nStep 4: Ryan does not flip the coin. Therefore, the coin is still heads up.\\nConclusion: Yes, the coin is still heads up.\\nQ: A coin is heads up. Lorena does not flip the coin. Shana does not flip the coin. Priscilla does not\\nflip the coin. Summer does not flip the coin. Is the coin still heads up? Note that \"flip\" here means\\n\"reverse\"\\nA: Let’s first understand the problem and devise a complete plan. Then, let’s carry out the plan and\\nreason problem step by step. Every step answer the subquestion, \"does the person flip and what is the\\ncoin’s current state?\". According to the coin’s last state, give the final answer (pay attention to every\\nflip and the coin’s turning state).\\nOutput:\\nPlan:\\nStep 1: Lorena does not flip the coin.\\nStep 2: Shana does not flip the coin.\\nStep 3: Priscilla does not flip the coin.\\nStep 4: Summer does not flip the coin.\\nAnswer:\\nStep 1: Lorena does not flip the coin. Therefore, the coin is still heads up.\\nStep 2: Shana does not flip the coin. Therefore, the coin is still heads up.\\nStep 3: Priscilla does not flip the coin. Therefore, the coin is still heads up.\\nStep 4: Summer does not flip the coin. Therefore, the coin is still heads up.\\nFinal Answer: Yes, the coin is still heads up.\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "link=\"https://arxiv.org/pdf/2305.04091\"\n",
    "def pdf_scrape(link):\n",
    "    loader = PyMuPDFLoader(link)\n",
    "    docs = loader.load()\n",
    "    pages_content =\"\"\n",
    "    for doc in docs:\n",
    "        pages_content += doc.page_content\n",
    "    return pages_content\n",
    "\n",
    "pdf_scrape(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content_from_url(soup):\n",
    "        \"\"\"Get the text from the soup\n",
    "\n",
    "        Args:\n",
    "            soup (BeautifulSoup): The soup to get the text from\n",
    "\n",
    "        Returns:\n",
    "            str: The text from the soup\n",
    "        \"\"\"\n",
    "        text = \"\"\n",
    "        tags = [\"p\", \"h1\", \"h2\", \"h3\", \"h4\", \"h5\"]\n",
    "        for element in soup.find_all(tags):  # Find all the <p> elements\n",
    "            text += element.text + \"\\n\"\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Sign up\\nSign in\\nSign up\\nSign in\\nSelf-hosting Llama 3 on a home server\\nDr James Ravenscroft\\nFollow\\n--\\n2\\nListen\\nShare\\nSelf-hosting Llama 3 as your own ChatGPT replacement service using a 10 year old graphics card and open source components.\\nLast week Meta launched Llama 3, the latest in their open source LLM series. Llama 3 is particularly interesting because the 8 billion parameter model, which is small enough to run on a laptop, performs as well as models 10x bigger than it. The responses it provides are as good as GPT-4 for many use cases.\\nI finally decided that this was motivation enough to dig out my old Nvidia Titan X card from the loft and slot it into my home server so that I could stand up a ChatGPT clone on my home network. In this post I explain some of the pros and cons of self-hosting llama 3 and provide configuration and resources to help you do it too.\\nHow it works\\nThe model is served by Ollama which is a GPU-enabled open source service for running LLMs as a service. Ollama makes heavy use of llama.cpp, t he same tech that I used to build turbopilot around 1 year ago. The frontend is powered by OpenWebUI which provides a ChatGPT-like user experience for interacting with Ollama models.\\nI use docker compose to run the two services and wire them together and I’ve got a Caddy web server set up to let in traffic from the outside world.\\nHardware\\nMy setup is running on a cheap and cheerful AMD CPU and Motherboard package and a 10 year old Nvidia Titan X card (much better GPUS are available on Ebay for around £150. The RTX 3060 with 12GB VRAM would be a great choice). My server has 32GB RAM but this software combo uses a lot less than that. You could probably get away with 16GB and run it smoothly or possibly even 8GB at a push.\\nYou could buy this bundle and a used RTX3060 on Ebay or a brand new one for around £250 and have a functional ChatGPT replacement in your house for less than £500.\\nPros and Cons of Llama 3\\nLlama 3 8B truly is a huge step forward for open source alternatives to relying on APIS from OpenAI, Anthropic and their peers. I am still in the early stages of working with my self-hosted Llama 3 instance but so far I’m finding that it is just as capable as GPT-4 in many arenas.\\nPro: Price\\nSelf-hosting Llama 3 with Ollama and OpenWebUI is free-ish except for any initial investment you need to make for hardware and then electricity consumption. ChatGPT plus is currently $20/month but techies are likely also burning a similar amount in API calls too. I already had all the components for this build lying around the house but if I bought them 2nd hand it would take around 1 year for them to pay for themselves. That said, I could massively increase my API consumption through my self-hosted models since it’s effectively “free”.\\nPro: Privacy\\nA huge advantage of this approach is that you’re not sending your data to an external company to be mined. The consumer version of ChatGPT that most people use is heavily data mined to improve OpenAI’s models and anything that you type in may end up in their corpus. Ollama runs entirely on your machine and never sends data back to any third party company.\\nPro: Energy Consumption and Carbon Footprint\\nAnother advantage is that since Llama 3:8B is small and it runs on a single GPU it uses a lot less energy to run than an average query to ChatGPT. My Titan X card consumes about 250 watts at max load but RTX 3060 cards only require 170 watts to run. Again, I had all the components lying around so I didn’t buy anything new to make this server and indeed it means I won’t be throwing away components that would otherwise become e-waste.\\nCon: Speed on old hardware\\nSelf-hosting Llama 3 8B on a Titan X is a little slower than ChatGPT but is still perfectly serviceable. It would almost certainly be faster on RTX 3 and 4 series cards.\\nCon: Multimodal Performance\\nThe biggest missing feature for me is currently multi-modal support. I use GPT-4 to do handwriting recognition and transcription for me and current gen open source models aren’t quite up to this yet. However, given the superb quality of Llama 3, I have no doubt that a similarly brilliant open multi-modal model is just around the corner.\\nCon: Training Transparency\\nAlthough Llama 3’s weights are free to download, the training corpus content is unknown. The model was built by Meta and thus is likely to have been trained on a large amount of user generated content and copyrighted content. Hosted third party models like ChatGPT are likely to be equally problematic in this regard but.\\nSetting up Llama 3 with Ollama and OpenWebUI\\nOnce you have the hardware assembled and the operating system installed, the fiddliest part is configuring Docker and Nvidia correctly.\\nUbuntu\\nIf you’re on Ubuntu, you’ll need to install docker first. I recommend using the guide from Docker themselves which installs the latest and greatest packages. Then follow this guide to install the nvidia runtime. Then you will want to verify that it’s all set up using the checking step below.\\nUnraid\\nI actually run Unraid on my home server rather than Ubuntu. To get things running there, simply install the unraid nvidia plugin through the community apps page and make sure to stop and start docker before trying out the step below.\\nChecking the Docker and Nvidia Setup (All OSes)\\nTo make sure that Docker and Nvidia are installed properly and able to talk to each other you can run:\\nThis runs the nvidia-smi status utility which should show what your GPU is currently doing but crucially it’s doing so from inside docker which means that nvidia’s container runtime is all set up to pass through the nvidia drivers to whatever you’re running inside your container. You should see something like this:\\nInstalling Ollama\\nCreate a new directory and a new empty text file called docker-compose.yml. Into that file paste the following:\\nWe define the two services and we provide both with volume mounts to enable them to persist data to disk (such as models you downloaded and your chat history).\\nFor now we leave ENABLE_SIGNUP commented out so that you can create an account in the web ui but later we can come back and turn that off so that internet denizens can’t sign up to use your chat.\\nTurn on Ollama\\nFirst we will turn on ollama and test it. Start by running docker-compose up -d ollama. (Depending on which version of docker you are running you might need to run docker compose rather than docker-compose). This will start just the ollama model server. We can interact with the model server by running an interactive chat session and downloading the model:\\nIn this command the first ollama refers to the container and ollama run llama3:8b is the command that will be executed inside the container. If all goes well you will see the server burst into action and download the llama3 model if this is the first time you've run it. You'll then be presented with an interactive prompt where you'll be able to chat to the model.\\nYou can press CTRL+D to quit and move on to the next step.\\nTurn on the Web UI\\nNow we will start up the web ui. Run docker-compose up -d ui. Now open up your browser and go to http://localhost:3011/ to see the web ui. You will need to register for an account and log in. After which you will be able to interact with the model like so:\\n(Optional) Configure Outside Access\\nIf you want to be able to chat to your models from the outside world you might want to stand up a reverse proxy to your server. If you’re new to self hosting and you’re not sure about how to do this, a safer option is probably to use Tailscale to build a VPN which you can use to securely connect to your home network without risking exposing your systems to the public and/or hackers.\\nConclusion\\nLlama 3 is a tremendously powerful model that is useful for a whole bunch of use cases including summarisation, creative brainstorming, code copiloting and more. The quality of the responses are in line with GPT-4 and it runs on much older, smaller hardware. Self-hosting Llama 3 won’t be for everyone and it’s quite technically involved. However, for AI geeks like me, running my own ChatGPT clone at home for next-to-nothing was too good an experiment to miss out on.\\nOriginally published at https://brainsteam.co.uk on April 20, 2024.\\n--\\n--\\n2\\nWritten by Dr James Ravenscroft\\nMl and NLP Geek, CTO at Filament. Saxophonist, foodie and explorer. I was born in Bermuda and I Live in the UK\\nHelp\\nStatus\\nAbout\\nCareers\\nBlog\\nPrivacy\\nTerms\\nText to speech\\nTeams\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    link=\"https://medium.com/@jamesravey/self-hosting-llama-3-on-a-home-server-00feeeba8174\"\n",
    "    session = requests.Session()\n",
    "    response = session.get(link, timeout=4)\n",
    "    soup = BeautifulSoup(\n",
    "        response.content, \"lxml\", from_encoding=response.encoding\n",
    "    )\n",
    "\n",
    "    for script_or_style in soup([\"script\", \"style\"]):\n",
    "        script_or_style.extract()\n",
    "\n",
    "    raw_content = get_content_from_url(soup)\n",
    "    lines = (line.strip() for line in raw_content.splitlines())\n",
    "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "    content = \"\\n\".join(chunk for chunk in chunks if chunk)\n",
    "    content\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error! : \" + str(e))\n",
    "\n",
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Self-hosting Llama 3 on a home server | by Dr James Ravenscroft | Apr, 2024 | MediumOpen in appSign upSign inWriteSign upSign inSelf-hosting Llama 3 on a home serverDr James Ravenscroft·Follow7 min read·Apr 20, 2024--2ListenShareSelf-hosting Llama 3 as your own ChatGPT replacement service using a 10 year old graphics card and open source components.Last week Meta launched Llama 3, the latest in their open source LLM series. Llama 3 is particularly interesting because the 8 billion parameter model, which is small enough to run on a laptop, performs as well as models 10x bigger than it. The responses it provides are as good as GPT-4 for many use cases.I finally decided that this was motivation enough to dig out my old Nvidia Titan X card from the loft and slot it into my home server so that I could stand up a ChatGPT clone on my home network. In this post I explain some of the pros and cons of self-hosting llama 3 and provide configuration and resources to help you do it too.How it worksThe model is served by Ollama which is a GPU-enabled open source service for running LLMs as a service. Ollama makes heavy use of llama.cpp, t he same tech that I used to build turbopilot around 1 year ago. The frontend is powered by OpenWebUI which provides a ChatGPT-like user experience for interacting with Ollama models.I use docker compose to run the two services and wire them together and I’ve got a Caddy web server set up to let in traffic from the outside world.HardwareMy setup is running on a cheap and cheerful AMD CPU and Motherboard package and a 10 year old Nvidia Titan X card (much better GPUS are available on Ebay for around £150. The RTX 3060 with 12GB VRAM would be a great choice). My server has 32GB RAM but this software combo uses a lot less than that. You could probably get away with 16GB and run it smoothly or possibly even 8GB at a push.You could buy this bundle and a used RTX3060 on Ebay or a brand new one for around £250 and have a functional ChatGPT replacement in your house for less than £500.Pros and Cons of Llama 3Llama 3 8B truly is a huge step forward for open source alternatives to relying on APIS from OpenAI, Anthropic and their peers. I am still in the early stages of working with my self-hosted Llama 3 instance but so far I’m finding that it is just as capable as GPT-4 in many arenas.Pro: PriceSelf-hosting Llama 3 with Ollama and OpenWebUI is free-ish except for any initial investment you need to make for hardware and then electricity consumption. ChatGPT plus is currently $20/month but techies are likely also burning a similar amount in API calls too. I already had all the components for this build lying around the house but if I bought them 2nd hand it would take around 1 year for them to pay for themselves. That said, I could massively increase my API consumption through my self-hosted models since it’s effectively “free”.Pro: PrivacyA huge advantage of this approach is that you’re not sending your data to an external company to be mined. The consumer version of ChatGPT that most people use is heavily data mined to improve OpenAI’s models and anything that you type in may end up in their corpus. Ollama runs entirely on your machine and never sends data back to any third party company.Pro: Energy Consumption and Carbon FootprintAnother advantage is that since Llama 3:8B is small and it runs on a single GPU it uses a lot less energy to run than an average query to ChatGPT. My Titan X card consumes about 250 watts at max load but RTX 3060 cards only require 170 watts to run. Again, I had all the components lying around so I didn’t buy anything new to make this server and indeed it means I won’t be throwing away components that would otherwise become e-waste.Con: Speed on old hardwareSelf-hosting Llama 3 8B on a Titan X is a little slower than ChatGPT but is still perfectly serviceable. It would almost certainly be faster on RTX 3 and 4 series cards.Con: Multimodal PerformanceThe biggest missing feature for me is currently multi-modal support. I use GPT-4 to do handwriting recognition and transcription for me and current gen open source models aren’t quite up to this yet. However, given the superb quality of Llama 3, I have no doubt that a similarly brilliant open multi-modal model is just around the corner.Con: Training TransparencyAlthough Llama 3’s weights are free to download, the training corpus content is unknown. The model was built by Meta and thus is likely to have been trained on a large amount of user generated content and copyrighted content. Hosted third party models like ChatGPT are likely to be equally problematic in this regard but.Setting up Llama 3 with Ollama and OpenWebUIOnce you have the hardware assembled and the operating system installed, the fiddliest part is configuring Docker and Nvidia correctly.UbuntuIf you’re on Ubuntu, you’ll need to install docker first. I recommend using the guide from Docker themselves which installs the latest and greatest packages. Then follow this guide to install the nvidia runtime. Then you will want to verify that it’s all set up using the checking step below.UnraidI actually run Unraid on my home server rather than Ubuntu. To get things running there, simply install the unraid nvidia plugin through the community apps page and make sure to stop and start docker before trying out the step below.Checking the Docker and Nvidia Setup (All OSes)To make sure that Docker and Nvidia are installed properly and able to talk to each other you can run:docker run --rm --runtime=nvidia --gpus all ubuntu nvidia-smiThis runs the nvidia-smi status utility which should show what your GPU is currently doing but crucially it’s doing so from inside docker which means that nvidia’s container runtime is all set up to pass through the nvidia drivers to whatever you’re running inside your container. You should see something like this:Installing OllamaCreate a new directory and a new empty text file called docker-compose.yml. Into that file paste the following:version: \"3.0\"services:  ui:    image: ghcr.io/open-webui/open-webui:main    restart: always    ports:      - 3011:8080    volumes:      - ./open-webui:/app/backend/data    environment:      # - \"ENABLE_SIGNUP=false\"      - \"OLLAMA_BASE_URL=http://ollama:11434\"  ollama:    image: ollama/ollama    restart: always    ports:      - 11434:11434    volumes:      - ./ollama:/root/.ollama    deploy:      resources:        reservations:          devices:            - driver: nvidia              count: 1              capabilities: [gpu]We define the two services and we provide both with volume mounts to enable them to persist data to disk (such as models you downloaded and your chat history).For now we leave ENABLE_SIGNUP commented out so that you can create an account in the web ui but later we can come back and turn that off so that internet denizens can’t sign up to use your chat.Turn on OllamaFirst we will turn on ollama and test it. Start by running docker-compose up -d ollama. (Depending on which version of docker you are running you might need to run docker compose rather than docker-compose). This will start just the ollama model server. We can interact with the model server by running an interactive chat session and downloading the model:docker-compose exec ollama ollama run llama3:8bIn this command the first ollama refers to the container and ollama run llama3:8b is the command that will be executed inside the container. If all goes well you will see the server burst into action and download the llama3 model if this is the first time you\\'ve run it. You\\'ll then be presented with an interactive prompt where you\\'ll be able to chat to the model.You can press CTRL+D to quit and move on to the next step.Turn on the Web UINow we will start up the web ui. Run docker-compose up -d ui. Now open up your browser and go to http://localhost:3011/ to see the web ui. You will need to register for an account and log in. After which you will be able to interact with the model like so:(Optional) Configure Outside AccessIf you want to be able to chat to your models from the outside world you might want to stand up a reverse proxy to your server. If you’re new to self hosting and you’re not sure about how to do this, a safer option is probably to use Tailscale to build a VPN which you can use to securely connect to your home network without risking exposing your systems to the public and/or hackers.ConclusionLlama 3 is a tremendously powerful model that is useful for a whole bunch of use cases including summarisation, creative brainstorming, code copiloting and more. The quality of the responses are in line with GPT-4 and it runs on much older, smaller hardware. Self-hosting Llama 3 won’t be for everyone and it’s quite technically involved. However, for AI geeks like me, running my own ChatGPT clone at home for next-to-nothing was too good an experiment to miss out on.Originally published at https://brainsteam.co.uk on April 20, 2024.AILlmSelf HostedOpen SourceChatGPT----2FollowWritten by Dr James Ravenscroft173 FollowersMl and NLP Geek, CTO at Filament. Saxophonist, foodie and explorer. I was born in Bermuda and I Live in the UKFollowHelpStatusAboutCareersBlogPrivacyTermsText to speechTeams\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Download the content\n",
    "link=\"https://medium.com/@jamesravey/self-hosting-llama-3-on-a-home-server-00feeeba8174\"\n",
    "def bs_scrape(link):\n",
    "    response = requests.get(link)\n",
    "    temp_file = f\"temp_{uuid.uuid4()}.html\"\n",
    "    with open(temp_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(response.text)\n",
    "    # Load it with an HTML parser\n",
    "    loader = BSHTMLLoader(temp_file)\n",
    "    document = loader.load()[0]\n",
    "    if os.path.exists(temp_file):\n",
    "        os.remove(temp_file)\n",
    "\n",
    "    # Clean up code\n",
    "    # Replace consecutive new lines with a single new line\n",
    "    document.page_content = re.sub(\"\\n\\n+\", \"\\n\", document.page_content)\n",
    "    return document.page_content\n",
    "\n",
    "bs_scrape(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Self-hosting Llama 3 on a home server | by Dr James Ravenscroft | Apr, 2024 | MediumOpen in appSign upSign inWriteSign upSign inSelf-hosting Llama 3 on a home serverDr James Ravenscroft·Follow7 min read·Apr 20, 2024--2ListenShareSelf-hosting Llama 3 as your own ChatGPT replacement service using a 10 year old graphics card and open source components.Last week Meta launched Llama 3, the latest in their open source LLM series. Llama 3 is particularly interesting because the 8 billion parameter model, which is small enough to run on a laptop, performs as well as models 10x bigger than it. The responses it provides are as good as GPT-4 for many use cases.I finally decided that this was motivation enough to dig out my old Nvidia Titan X card from the loft and slot it into my home server so that I could stand up a ChatGPT clone on my home network. In this post I explain some of the pros and cons of self-hosting llama 3 and provide configuration and resources to help you do it too.How it worksThe model is served by Ollama which is a GPU-enabled open source service for running LLMs as a service. Ollama makes heavy use of llama.cpp, t he same tech that I used to build turbopilot around 1 year ago. The frontend is powered by OpenWebUI which provides a ChatGPT-like user experience for interacting with Ollama models.I use docker compose to run the two services and wire them together and I’ve got a Caddy web server set up to let in traffic from the outside world.HardwareMy setup is running on a cheap and cheerful AMD CPU and Motherboard package and a 10 year old Nvidia Titan X card (much better GPUS are available on Ebay for around £150. The RTX 3060 with 12GB VRAM would be a great choice). My server has 32GB RAM but this software combo uses a lot less than that. You could probably get away with 16GB and run it smoothly or possibly even 8GB at a push.You could buy this bundle and a used RTX3060 on Ebay or a brand new one for around £250 and have a functional ChatGPT replacement in your house for less than £500.Pros and Cons of Llama 3Llama 3 8B truly is a huge step forward for open source alternatives to relying on APIS from OpenAI, Anthropic and their peers. I am still in the early stages of working with my self-hosted Llama 3 instance but so far I’m finding that it is just as capable as GPT-4 in many arenas.Pro: PriceSelf-hosting Llama 3 with Ollama and OpenWebUI is free-ish except for any initial investment you need to make for hardware and then electricity consumption. ChatGPT plus is currently $20/month but techies are likely also burning a similar amount in API calls too. I already had all the components for this build lying around the house but if I bought them 2nd hand it would take around 1 year for them to pay for themselves. That said, I could massively increase my API consumption through my self-hosted models since it’s effectively “free”.Pro: PrivacyA huge advantage of this approach is that you’re not sending your data to an external company to be mined. The consumer version of ChatGPT that most people use is heavily data mined to improve OpenAI’s models and anything that you type in may end up in their corpus. Ollama runs entirely on your machine and never sends data back to any third party company.Pro: Energy Consumption and Carbon FootprintAnother advantage is that since Llama 3:8B is small and it runs on a single GPU it uses a lot less energy to run than an average query to ChatGPT. My Titan X card consumes about 250 watts at max load but RTX 3060 cards only require 170 watts to run. Again, I had all the components lying around so I didn’t buy anything new to make this server and indeed it means I won’t be throwing away components that would otherwise become e-waste.Con: Speed on old hardwareSelf-hosting Llama 3 8B on a Titan X is a little slower than ChatGPT but is still perfectly serviceable. It would almost certainly be faster on RTX 3 and 4 series cards.Con: Multimodal PerformanceThe biggest missing feature for me is currently multi-modal support. I use GPT-4 to do handwriting recognition and transcription for me and current gen open source models aren’t quite up to this yet. However, given the superb quality of Llama 3, I have no doubt that a similarly brilliant open multi-modal model is just around the corner.Con: Training TransparencyAlthough Llama 3’s weights are free to download, the training corpus content is unknown. The model was built by Meta and thus is likely to have been trained on a large amount of user generated content and copyrighted content. Hosted third party models like ChatGPT are likely to be equally problematic in this regard but.Setting up Llama 3 with Ollama and OpenWebUIOnce you have the hardware assembled and the operating system installed, the fiddliest part is configuring Docker and Nvidia correctly.UbuntuIf you’re on Ubuntu, you’ll need to install docker first. I recommend using the guide from Docker themselves which installs the latest and greatest packages. Then follow this guide to install the nvidia runtime. Then you will want to verify that it’s all set up using the checking step below.UnraidI actually run Unraid on my home server rather than Ubuntu. To get things running there, simply install the unraid nvidia plugin through the community apps page and make sure to stop and start docker before trying out the step below.Checking the Docker and Nvidia Setup (All OSes)To make sure that Docker and Nvidia are installed properly and able to talk to each other you can run:docker run --rm --runtime=nvidia --gpus all ubuntu nvidia-smiThis runs the nvidia-smi status utility which should show what your GPU is currently doing but crucially it’s doing so from inside docker which means that nvidia’s container runtime is all set up to pass through the nvidia drivers to whatever you’re running inside your container. You should see something like this:Installing OllamaCreate a new directory and a new empty text file called docker-compose.yml. Into that file paste the following:version: \"3.0\"services:  ui:    image: ghcr.io/open-webui/open-webui:main    restart: always    ports:      - 3011:8080    volumes:      - ./open-webui:/app/backend/data    environment:      # - \"ENABLE_SIGNUP=false\"      - \"OLLAMA_BASE_URL=http://ollama:11434\"  ollama:    image: ollama/ollama    restart: always    ports:      - 11434:11434    volumes:      - ./ollama:/root/.ollama    deploy:      resources:        reservations:          devices:            - driver: nvidia              count: 1              capabilities: [gpu]We define the two services and we provide both with volume mounts to enable them to persist data to disk (such as models you downloaded and your chat history).For now we leave ENABLE_SIGNUP commented out so that you can create an account in the web ui but later we can come back and turn that off so that internet denizens can’t sign up to use your chat.Turn on OllamaFirst we will turn on ollama and test it. Start by running docker-compose up -d ollama. (Depending on which version of docker you are running you might need to run docker compose rather than docker-compose). This will start just the ollama model server. We can interact with the model server by running an interactive chat session and downloading the model:docker-compose exec ollama ollama run llama3:8bIn this command the first ollama refers to the container and ollama run llama3:8b is the command that will be executed inside the container. If all goes well you will see the server burst into action and download the llama3 model if this is the first time you\\'ve run it. You\\'ll then be presented with an interactive prompt where you\\'ll be able to chat to the model.You can press CTRL+D to quit and move on to the next step.Turn on the Web UINow we will start up the web ui. Run docker-compose up -d ui. Now open up your browser and go to http://localhost:3011/ to see the web ui. You will need to register for an account and log in. After which you will be able to interact with the model like so:(Optional) Configure Outside AccessIf you want to be able to chat to your models from the outside world you might want to stand up a reverse proxy to your server. If you’re new to self hosting and you’re not sure about how to do this, a safer option is probably to use Tailscale to build a VPN which you can use to securely connect to your home network without risking exposing your systems to the public and/or hackers.ConclusionLlama 3 is a tremendously powerful model that is useful for a whole bunch of use cases including summarisation, creative brainstorming, code copiloting and more. The quality of the responses are in line with GPT-4 and it runs on much older, smaller hardware. Self-hosting Llama 3 won’t be for everyone and it’s quite technically involved. However, for AI geeks like me, running my own ChatGPT clone at home for next-to-nothing was too good an experiment to miss out on.Originally published at https://brainsteam.co.uk on April 20, 2024.AILlmSelf HostedOpen SourceChatGPT----2FollowWritten by Dr James Ravenscroft173 FollowersMl and NLP Geek, CTO at Filament. Saxophonist, foodie and explorer. I was born in Bermuda and I Live in the UKFollowHelpStatusAboutCareersBlogPrivacyTermsText to speechTeams\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from requests.packages.urllib3.exceptions import InsecureRequestWarning\n",
    "requests.packages.urllib3.disable_warnings(InsecureRequestWarning)\n",
    "\n",
    "\n",
    "link=\"https://medium.com/@jamesravey/self-hosting-llama-3-on-a-home-server-00feeeba8174\"\n",
    "def web_scrape(link):\n",
    "    try:\n",
    "        loader = WebBaseLoader(link)\n",
    "        loader.requests_kwargs = {\"verify\": False}\n",
    "        docs = loader.load()\n",
    "        content = \"\"\n",
    "        for doc in docs:\n",
    "            content += doc.page_content\n",
    "\n",
    "        return content\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error! : \" + str(e))\n",
    "        return \"\"\n",
    "\n",
    "web_scrape(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dan-dev-langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
