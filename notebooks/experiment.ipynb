{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append('/Users/dan/Code/LLM/llm_analyst')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_analyst.core.config import Config\n",
    "from llm_analyst.core.research_analyst import LLMAnalyst\n",
    "query = \"What happened in the latest burning man floods?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dan/miniforge3/envs/gpt-researcher/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "active_research_topic  = What happened in the latest burning man floods?\n",
      "main_research_topic    = \n",
      "Visited URLs length    = 12\n",
      "Reseach finding length = 4\n",
      "agent_type             = ðŸŒŽ News Reporter Agent\n",
      "agents_role_prompt     = You are an experienced AI news reporter assistant. Your main goal is to compose comprehensive, precise, unbiased, and methodically arranged news reports based on provided data and events.\n",
      "\n",
      "['Overview of the Burning Man Floods', 'Impact on Festival Goers and Death Report', 'Weather Conditions and Exodus Operations']\n"
     ]
    }
   ],
   "source": [
    "config_params = {}\n",
    "config_params = {\n",
    "    \"internet_search\" :{\"default_val\":\"ddg_search\"},\n",
    "    \"llm_provider\"    :{\"default_val\":\"groq\"},\n",
    "    \"llm_model\"       :{\"default_val\":\"mixtral-8x7b-32768\"},\n",
    "}\n",
    "\n",
    "#    \"llm_model\"       :{\"default_val\":\"mixtral-8x7b-32768\"},\n",
    "#    \"llm_model\"       :{\"default_val\":\"gemma-7b-it\"},\n",
    "#    \"llm_model\"       :{\"default_val\":\"llama3-8b-8192\"},\n",
    "#    \"llm_model\"       :{\"default_val\":\"llama3-70b-8192\"},\n",
    "#    \"llm_model\"       :{\"default_val\":\"gpt-4o-2024-05-13\"},\n",
    "\n",
    "\n",
    "config = Config()\n",
    "config._set_values_for_config(config_params)\n",
    "llm_analyst = LLMAnalyst(query,config = config)\n",
    "primary_research = choose_agent_dict = await llm_analyst.conduct_research()\n",
    "# choose_agent_dict = await llm_analyst.choose_agent()\n",
    "subtopics = await llm_analyst.select_subtopics()\n",
    "# subqueries_dict = await llm_analyst.get_sub_queries()\n",
    "# print(subtopics_dict)\n",
    "# print(subqueries_dict)\n",
    "# print(\"=\"*40)\n",
    "# print(llm_analyst.cfg)\n",
    "print(primary_research)\n",
    "print(subtopics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " for subtopic in subtopics:\n",
    "     subtopic_assistant = LLMAnalyst(\n",
    "        active_research_topic = subtopic,\n",
    "        report_type = \"subtopic_report\",\n",
    "        main_research_topic = primary_research.active_research_topic,\n",
    "        visited_urls = primary_research.visited_urls,\n",
    "        agents_role_prompt =  primary_research.agent_type,\n",
    "        agents_type = primary_research.agents_role_prompt\n",
    "    )\n",
    "     subtopic_assistant.research_findings = primary_research.research_findings\n",
    "     subtopic_assistant.report_headings = primary_research.report_headings\n",
    "     \n",
    "     subtopic_research = await subtopic_assistant.conduct_research()\n",
    "     subtopic_report = await subtopic_assistant.write_report()\n",
    "\n",
    "     \n",
    "     \n",
    "        result = await get_subtopic_report(query, subtopic, main_task_assistant.agent, main_task_assistant.role, global_context, global_urls, existing_headers)\n",
    "        if result[\"report\"]:\n",
    "            subtopic_reports.append(result)\n",
    "            subtopics_report_body += \"\\n\\n\\n\" + result[\"report\"]\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtopics_dict = await llm_analyst.select_subtopics()\n",
    "subtopics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Serialize the list\n",
    "with open('llm_analyst_context.json', 'w') as file:\n",
    "     json.dump(llm_analyst.context, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('llm_analyst_context.json', 'r') as file:\n",
    "    loaded_list = json.load(file)\n",
    "\n",
    "print(loaded_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_analyst.core.research_analyst import LLMAnalyst\n",
    "QUERY    = \"What happened in the latest burning man floods?\"\n",
    "llm_analyst2 = LLMAnalyst(QUERY)\n",
    "llm_analyst2.role = \"You are a well-informed AI news analyst assistant. Your primary goal is to provide comprehensive, accurate, unbiased, and well-structured news reports based on the latest events and developments.\"\n",
    "llm_analyst2.context = loaded_list\n",
    "llm_analyst2.context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = await llm_analyst2.write_report()\n",
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "prompt_keys = prompts._prompts.keys()\n",
    "for prompt_key in prompt_keys:\n",
    "    prompt = prompts.get_prompt(prompt_key)\n",
    "    variables = re.findall(r'\\{(.*?)\\}', prompt)\n",
    "    variables = [f\"{variable}={variable}\" for variable in list(variables)]\n",
    "    params = ', '.join(variables)\n",
    "    fun_call=f\"prompts.get_prompt('{prompt_key}',{params})\"\n",
    "    print(fun_call)\n",
    "\n",
    "    #print(\"=\"*20,prompt_key,\"=\"*20)\n",
    "    #print(f\"{prompt_key}({variables})\")\n",
    "    #print(prompt)\n",
    "    #print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_analyst.core.prompts import Prompts\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "context = \"This context\"\n",
    "question = \"What is the meaning of life?\"\n",
    "total_words = 2000\n",
    "report_format = \"APA\"\n",
    "datetime_now = datetime.now().strftime('%B %d, %Y')\n",
    "\n",
    "prompts = Prompts()\n",
    "prompt = prompts.get_prompt(\"generate_report_prompt\",\n",
    "                            context=context,\n",
    "                            question=question,\n",
    "                            total_words=total_words,\n",
    "                            report_format=report_format,\n",
    "                            datetime_now=datetime_now)\n",
    "print(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_analyst.core.prompts import Prompts\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "context = \"This context\"\n",
    "question = \"What is the meaning of life?\"\n",
    "total_words = 2000\n",
    "task = \"What is the meaning of life?\"\n",
    "max_iterations = 5\n",
    "datetime_now = datetime.now().strftime('%B %d, %Y')\n",
    "\n",
    "prompts = Prompts()\n",
    "prompt = prompts.get_prompt(\"generate_resource_report_prompt\",\n",
    "                            context=context,\n",
    "                            question=question,\n",
    "                            total_words=total_words)\n",
    "print(prompt)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from llm_analyst.core.prompts import Prompts\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "prompts = Prompts()\n",
    "prompt = prompts.get_prompt(\"auto_agent_instructions\")\n",
    "print(prompt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_analyst.core.researcher import LLMAnalyst\n",
    "query = \"What happened in the latest burning man floods?\"\n",
    "llm_analyst = LLMAnalyst(query)\n",
    "agent, agent_prompt = await llm_analyst.choose_agent()\n",
    "print(f\"Agent={agent}\\nAgent Prompt={agent_prompt}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "def my_trace_log(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        function_name = inspect.currentframe().f_code.co_name\n",
    "        print(f\"TRACE: Entering {func.__name__}\")\n",
    "        result = func(*args, **kwargs)\n",
    "        print(f\"TRACE: Exiting {func.__name__}\")\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "@my_trace_log\n",
    "def hello_world():\n",
    "    print(\"Hello World\")\n",
    "\n",
    "# Example usage\n",
    "hello_world()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_groq import ChatGroq\n",
    "from llm_analyst.core.exceptions import LLMAnalystsException\n",
    "deterministic_temp=0\n",
    "role = \"You are a well-informed AI news analyst assistant. Your primary goal is to provide comprehensive, accurate, unbiased, and well-structured news reports based on the latest events and developments.\"\n",
    "report_prompt = \"Information: context data....\\n\\nUsing the above information, answer the following query or task: 'What happened in the latest burning man floods?' in a detailed report -- The report should focus on the answer to the query, should be well structured, informative, in-depth and comprehensive, with facts and numbers if available and a minimum of 1000 words.\\nYou should strive to write the report as long as you can using all relevant and necessary information provided.\\nYou must write the report with markdown syntax.\\nUse an unbiased and journalistic tone.\\nYou MUST determine your own concrete and valid opinion based on the given information. Do NOT deter to general and meaningless conclusions.\\nYou MUST write all used source urls at the end of the report as references, and make sure not to add duplicated sources, but only one reference for each.\\nEvery url should be hyperlinked: [url website](url)\\nAdditionally, you MUST include hyperlinks to the relevant URLs wherever they are referenced in the report : \\n\\n       e.g.:\\n            # Report Header\\n\\n            This is a sample text. ([url website](url))\\n\\nYou MUST write the report in APA format.\\nCite search results using inline notations.\\nOnly cite the most relevant results that answer the query accurately.\\nPlace these citations at the end of the sentence or paragraph that reference them.\\nPlease do your best, this is very important to my career.\\nAssume that the current date is May 21, 2024\"\n",
    "messages=[\n",
    "        {\"role\": \"system\", \"content\": \"role\"},\n",
    "        {\"role\": \"user\",   \"content\": \"report_prompt\"}]\n",
    "            \n",
    "#api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "api_key = os.environ[\"GROQ_API_KEY\"]\n",
    "temperature = 0\n",
    "#model = \"gpt-4o-2024-05-13\"\n",
    "model = \"llama3-70b-8192\"\n",
    "max_tokens=4000\n",
    "llm = ChatGroq(\n",
    "            model = model,\n",
    "            temperature = temperature,\n",
    "            max_tokens = max_tokens,\n",
    "            api_key = api_key\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = await llm.ainvoke(messages)\n",
    "response = output.content\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_utf8 = report.encode('utf-8', errors='replace').decode('utf-8')\n",
    "filename=\"report_markdown.md\"\n",
    "with open(filename, \"w\", encoding='utf-8') as file:\n",
    "  # Directly write the report content to the file, avoiding encoding/decoding\n",
    "  file.write(text_utf8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from groq import Groq\n",
    "\n",
    "client = Groq(\n",
    "    api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Explain the importance of fast language models\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama3-70b-8192\",\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "class Subtopic(BaseModel):\n",
    "    task: str = Field(description=\"Task name\", min_length=1)\n",
    "\n",
    "class Subtopics(BaseModel):\n",
    "    subtopics: List[Subtopic] = []\n",
    "    \n",
    "parser = PydanticOutputParser(pydantic_object=Subtopics)\n",
    "parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt-researcher",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
